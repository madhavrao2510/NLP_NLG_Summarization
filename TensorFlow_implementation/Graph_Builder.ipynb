{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this Notebook, I'll write the script for building the Order-Planner Model defined in the base referenced paper.\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "The jupyter allows for a very easy graph building process while using the tf.InteractiveSession(). It is almost as if we are using the eager execution strategy. [Note it is not exactly same as eager execution. we have to explicitly write <i> tenosr.eval() </i> for execution.]\n",
    "link to paper -> https://arxiv.org/abs/1709.00155\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "# Technology used: Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as usual, I'll start with the utility cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages used for processing: \n",
    "import matplotlib.pyplot as plt # for visualization\n",
    "import numpy as np\n",
    "\n",
    "# for operating system related stuff\n",
    "import os\n",
    "import sys # for memory usage of objects\n",
    "from subprocess import check_output\n",
    "\n",
    "# The tensorflow_graph_package for this implementation\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.pickling_tools import *\n",
    "\n",
    "# import tensorflow temporarily:\n",
    "import tensorflow as tf\n",
    "#from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import tf_slim as slim\n",
    "\n",
    "# to plot the images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the \"../Data/\" directory.\n",
    "\n",
    "def exec_command(cmd):\n",
    "    '''\n",
    "        function to execute a shell command and see it's \n",
    "        output in the python console\n",
    "        @params\n",
    "        cmd = the command to be executed along with the arguments\n",
    "              ex: ['ls', '../input']\n",
    "    '''\n",
    "    print(check_output(cmd).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\n",
      "ERROR: No matching distribution found for pickle\n",
      "WARNING: You are using pip version 21.3.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\anusa\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.7.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (4.40.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (3.0.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.15.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\anusa\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\anusa\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33640/3229833260.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# check the structure of the project directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mexec_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ls'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33640/1693678355.py\u001b[0m in \u001b[0;36mexec_command\u001b[1;34m(cmd)\u001b[0m\n\u001b[0;32m      9\u001b[0m               \u001b[0mex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ls'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'../input'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     '''\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[0;32m    425\u001b[0m                **kwargs).stdout\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stderr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 505\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    506\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 951\u001b[1;33m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[0;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m             \u001b[1;31m# Start the process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m                 hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n\u001b[0m\u001b[0;32m   1421\u001b[0m                                          \u001b[1;31m# no special security\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m                                          \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "# check the structure of the project directory\n",
    "exec_command(['ls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(3) # set this seed for a device independant consistent behaviour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Set the constants for the script '''\n",
    "\n",
    "# various paths of the files\n",
    "data_path = \"../Data\" # the data path\n",
    "\n",
    "data_files_paths = {\n",
    "    \"table_content\": os.path.join(data_path, \"train.box\"),\n",
    "    \"nb_sentences\" : os.path.join(data_path, \"train.nb\"),\n",
    "    \"train_sentences\": os.path.join(data_path, \"train.sent\")\n",
    "}\n",
    "\n",
    "base_model_path = \"Models\"\n",
    "plug_and_play_data_file = os.path.join(data_path, \"plug_and_play.pickle\")\n",
    "\n",
    "# constants for this script\n",
    "train_percentage = 90\n",
    "learning_rate = 3e-4 # for learning rate -> https://twitter.com/karpathy/status/801621764144971776?lang=en\n",
    "# I know the tweet was a joke, but I have noticed that this learning rate works quite well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Data\\\\plug_and_play.pickle'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plug_and_play_data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\x80\\x02}q\\x01(U\\x10field_vocab_sizeKjU\\nfield_dictq\\x02}q\\x03(K\\x00U\\x05<unk>q\\x04K\\x01U\\x05yearsq\\x05K\\x02U\\x04nameq\\x06K\\x03U\\nbirthplaceq\\x07K\\x04U\\tbirthdateq\\x08K\\x05U\\x07captionq\\tK\\x06U\\x0carticletitleq\\nK\\x07U\\x05imageq\\x0bK\\x08U\\x05clubsq\\x0cK\\tU\\ndeathplaceq\\rK\\nU\\x05teamsq\\x0eK\\x0bU\\x05labelq\\x0fK\\x0cU\\x0eassociatedactsq\\x10K\\rU\\x05titleq\\x11K\\x0eU\\x08feastdayq\\x12K\\x0fU\\x04capsq\\x13K\\x10U\\x05goalsq\\x14K\\x11U\\tstatlabelq\\x15K\\x12U\\tdeathdateq\\x16K\\x13U\\rbeatifieddateq\\x17K\\x14U\\tresidenceq\\x18K\\x15U\\x08pcupdateq\\x19K\\x16U\\x06titlesq\\x1aK\\x17U\\x06buriedq\\x1bK\\x18U\\x05coachq\\x1cK\\x19U\\x0byearsactiveq\\x1dK\\x1aU\\x06spouseq\\x1eK\\x1bU\\tsuccessorq\\x1fK\\x1cU\\talmamaterq K\\x1dU\\x08fullnameq!K\\x1eU\\x08positionq\"K\\x1fU\\x0bpredecessorq#K U\\x08religionq$K!U\\timagesizeq%K\"U\\tdebutteamq&K#U\\x08statyearq\\'K$U\\x05genreq(K%U\\tenthronedq)K&U\\x05endedq*K\\'U\\x06heightq+K(U\\x05eventq,K)U\\x14dateofhighestrankingq-K*U\\x14dateofcurrentrankingq.K+U\\x07updatedq/K,U\\x04teamq0K-U\\tstatvalueq1K.U\\x06originq2K/U\\x0bveneratedinq3K0U\\x0bbeatifiedbyq4K1U\\tpatronageq5K2U\\x0bnationalityq6K3U\\x06weightq7K4U\\x0emedaltemplatesq8K5U\\x0bcurrentclubq9K6U\\x05playsq:K7U\\x07websiteq;K8U\\x0ehighestrankingq<K9U\\x0ecurrentrankingq=K:U\\noccupationq>K;U\\x06numberq?K<U\\tdebutdateq@K=U\\x07collegeqAK>U\\x04typeqBK?U\\nordinationqCK@U\\x0cconsecrationqDKAU\\tbirthnameqEKBU\\tsignatureqFKCU\\teducationqGKDU\\x0cplayingstyleqHKEU\\nclubnumberqIKFU\\x08nicknameqJKGU\\x07countryqKKHU\\tturnedproqLKIU\\x07retiredqMKJU\\x07racquetqNKKU\\x0fworldopenresultqOKLU\\x06finalsqPKMU\\nstatehouseqQKNU\\x08districtqRKOU\\ttermstartqSKPU\\x07termendqTKQU\\x0cconstituencyqUKRU\\x05partyqVKSU\\x08childrenqWKTU\\x03altqXKUU\\x04batsqYKVU\\x06throwsqZKWU\\tdebutyearq[KXU\\x06awardsq\\\\KYU\\nbackgroundq]KZU\\x08heightftq^K[U\\x08heightinq_K\\\\U\\tweightlbsq`K]U\\rundraftedyearqaK^U\\x05statsqbK_U\\x10databasefootballqcK`U\\x03pfrqdKaU\\x08probowlsqeKbU\\x0ebeatifiedplaceqfKcU\\rcanonizeddateqgKdU\\x0ecanonizedplaceqhKeU\\x0bcanonizedbyqiKfU\\nattributesqjKgU\\x0bmajorshrineqkKhU\\x0esuppresseddateqlKiU\\x06issuesqmuU\\x0ffield_encodings]qn(]qo(K>K\\x02K\\x02K\\x02K\\x02K\\rK\\rK\\rK\\rK\\rK\\rK\\rK\\rK\\rK\\rK\\rK\\rK\\x07K\\x05K%K%K%K&K&K&K\\x1fK\\x1fK\\x1bK\\x1bK?K@K\\x04KAK\\x03K\\x12K\\x12K\\x12K\\x17K\\x17K\\x17K\\x17K\\x17K\\x17K2K K K K\\x14K\\x14K\\x14K\\x14K\\x0eK\\x0eK\\x0eK\\x0eK\\x0eK\\x0eK\\x0eK\\x0eK\\x0eK\\x0eK\\x1cKBK\\x06K\\x06K\\x06K\\x06K\\x06e]qp(K\\x02K\\x02K\\x07K!K\\x05K\\x1dK\\x1dKCK2KDK\\x04K\\x03K\\x12K\\tK\\'K3K4K\\x06K\\x06e]qq(K\\x02K\\x02K\\x07K\\x1dK\\x1dK\\'K\\x04K\\x04K\\x04K\\x03K\\x03K\\x03K5K5KEK\\x1eK\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x08K\\x0fK\\x0fK\\x0fK\\x0fK\\x0fK\\x0fK\\x0fK\\x0fK\\x0fK\\x0fK\\x0fK\\x10K\\x10K\\x10K\\x10K\\x10K\\x10K\\x10K\\x10K\\x10K\\x10K\\x10K\\x15K\\x15K\\x15K\\x15K\\x15K\\x15K\\x15K\\x15K\\x06K\\x06e]qr(K\\x02K\\x02K\\x07K\\x05K\\x1dKFKGK\\x14K\\x14K\\x14K\\x04K\\x04K\\x04K\\x03K\\x12K\\tK\\'K3KHKIK6K6K\\x18K\\x18K\\x18K\\x18K\\x18K\\x18KJK7K(K(K(K\\x19K8K8K)K)K)K9K9K*K*K*KKK\\x16KLK4K+K+K+K\\x06K\\x06e]qs(K\\x07K\\x02K\\x02K\\x02KMKNKOKPK\\x1fK\\x1fK\\x1bK\\x1bK\\x1bKQK\\x04K\\x04K\\x04K\\x03K\\x03K\\x03K\\x12K\\tKRK\\x1cK\\x1cK\\x1cK\\x1cK:K\\x1aK\\x1aK\\x1aK\\x1aK\\x1aK\\x1aKSK\\x14K K7K\\x06K\\x06e]qt(K\\x07K\\x07K\\x07KTK\\x05K\\x05K\\x05K\\x02K\\x02K\\x04K\\x04K\\x04K\\x03K\\x03K\\x03K\\x03K\\x03K\\x12K\\tK:K\\x19K\\x19K\\x19K\\x06K\\x06e]qu(K\\x02K\\x02K\\x07K\\x07K\\x07K\\x07K\\x07K\\x07K\\x07K!K\\x05K\\x05K\\x05K\\x05K\\x05K\\x05K\\x1eK\\x1eK,K,K,K;KUKVK\\x04K\\x04K\\x04K\\x03K\\x03K\\x03K<K<KWK\"K\"K\"K\"K#K#K#K#K\\x11K\\x11K\\x11K\\x11K\\x11K\\x11K\\x11K\\x11K\\x11K\\x11K\\x11K-K-K-K\\nKXK\\x06K\\x06e]qv(K\\x07K\\x07K\\x07K\\x07K\\x07K\\x05K\\x05K\\x05K\\x05K\\x05K\\x05K\\x05K\\x05K\\x02K\\x02K!KYK\\x04K\\x04K\\x04K.K.K.K$K$K$K$K\\x19K\\x19K\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0bK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x0cK\\x06K\\x06e]qw(K\\x07K\\x04K\\x04K\\x04K\\x03K\\x03K\\x03K\\x1eK\\x1eK;K=K=KZK[K\\\\K]K^K_K`KaK\\x01K\\x01K\\nK\\nK\\nK\\nK\\nK\\nK\\nK\\nK\\nK\\nK\\nK\\nK\\nK\\x06K\\x06e]qx(K\\x02K\\x02K\\x02K\\x02K\\x02K\\x02K\\x02K\\x02K\\x04K\\x04K\\x04K\\x12K\\x12K\\x12K\\x0eK\\x0eK/K/K/K\\x07K!K\\x05K\\x05K\\x05K\\x05K\\x03K\\x03K\\x03K\\x03K\\x03K\\x03K\\x03K\\tK\\tK\\tK\\tK\\tK\\tK\\tK\\tK\\tK\\tK\\tK\\tK\\x16K\\x16K\\x16K\\x16K\\x16K\\x16K\\x13K\\x13K\\x13K\\x13K\\x13K\\x13K\\x13K\\x13K\\x13KbK0K0K0KcKdKeKfK1K1K1KgKhKiK\\x06K\\x06K\\x06eeU\\x18content_label_vocab_sizeM\\xa5\\x01U\\x18content_union_label_dict}qy(K\\x00h\\x04K\\x01U\\x06<none>qzK\\x02U\\x01,K\\x03U\\x02ofq{K\\x04U\\x05-lrb-q|K\\x05U\\x05-rrb-q}K\\x06U\\x01.K\\x07U\\x03theq~K\\x08U\\x02--q\\x7fK\\tU\\x01aK\\nU\\x02inq\\x80K\\x0bU\\x03andq\\x81K\\x0cU\\x07<start>q\\x82K\\rU\\x05<eos>q\\x83K\\x0eU\\x02isq\\x84K\\x0fU\\x03wasq\\x85K\\x10U\\x08novemberq\\x86K\\x11U\\x08crawfordq\\x87K\\x12U\\x04bornq\\x88K\\x13U\\x04popeq\\x89K\\x14U\\x042015q\\x8aK\\x15U\\x03jimq\\x8bK\\x16U\\x07cattaroq\\x8cK\\x17U\\nalexandriaq\\x8dK\\x18U\\x05aprilq\\x8eK\\x19U\\x05marchq\\x8fK\\x1aU\\x0220q\\x90K\\x1bU\\x04jensq\\x91K\\x1cU\\x03bobq\\x92K\\x1dU\\x02toq\\x93K\\x1eU\\x03sheq\\x94K\\x1fU\\x07michaelq\\x95K U\\x03iiiq\\x96K!U\\x0216q\\x97K\"U\\x03huiq\\x98K#U\\x03junq\\x99K$U\\x04okanq\\x9aK%U\\x08\\xc3\\x96zt\\xc3\\xbcrkq\\x9bK&U\\x041996q\\x9cK\\'U\\x041999q\\x9dK(U\\x04julyq\\x9eK)U\\x07martinoq\\x9fK*U\\x06salomeq\\xa0K+U\\x04carlq\\xa1K,U\\x04leftq\\xa2K-U\\x03sanq\\xa3K.U\\x06osannaq\\xa4K/U\\x05kotorq\\xa5K0U\\x02asq\\xa6K1U\\x04fromq\\xa7K2U\\x0cprofessionalq\\xa8K3U\\x02heq\\xa9K4U\\x08baseballq\\xaaK5U\\x06leagueq\\xabK6U\\x04markq\\xacK7U\\x03907q\\xadK8U\\x05saintq\\xaeK9U\\x06copticq\\xafK:U\\x02\\'sq\\xb0K;U\\x06churchq\\xb1K<U\\x04yeniq\\xb2K=U\\x0bmalatyasporq\\xb3K>U\\x041998q\\xb4K?U\\x018K@U\\x015KAU\\x013KBU\\x0214q\\xb5KCU\\x05marieq\\xb6KDU\\x07stephanq\\xb7KEU\\x06franceq\\xb8KFU\\x03no.q\\xb9KGU\\x03101q\\xbaKHU\\x011KIU\\x07leonardq\\xbbKJU\\x0cpennsylvaniaq\\xbcKKU\\x02onq\\xbdKLU\\x03losq\\xbeKMU\\x07angelesq\\xbfKNU\\x07dodgersq\\xc0KOU\\x06augustq\\xc1KPU\\x02atq\\xc2KQU\\x07englandq\\xc3KRU\\x07blessedq\\xc4KSU\\x05knownq\\xc5KTU\\x06formerq\\xc6KUU\\x06playerq\\xc7KVU\\x03forq\\xc8KWU\\x08americanq\\xc9KXU\\x02\\'\\'q\\xcaKYU\\x05majorq\\xcbKZU\\tpatriarchq\\xccK[U\\x03seeq\\xcdK\\\\U\\x03st.q\\xceK]U\\x0225q\\xcfK^U\\x03880q\\xd0K_U\\x01iK`U\\x05egyptq\\xd1KaU\\x0230q\\xd2KbU\\x041977q\\xd3KcU\\x07strikerq\\xd4KdU\\x041995q\\xd5KeU\\x042001q\\xd6KfU\\x042006q\\xd7KgU\\x042008q\\xd8KhU\\x042009q\\xd9KiU\\x042011q\\xdaKjU\\x042012q\\xdbKkU\\x07\\xc3\\x87aykurq\\xdcKlU\\x08rizesporq\\xddKmU\\x010KnU\\x0212q\\xdeKoU\\x06activeq\\xdfKpU\\x02l.q\\xe0KqU\\x06butlerq\\xe1KrU\\ndemocraticq\\xe2KsU\\nuniversityq\\xe3KtU\\x03mayq\\xe4KuU\\x041935q\\xe5KvU\\x07actressq\\xe6KwU\\x0221q\\xe7KxU\\x04withq\\xe8KyU\\x07fielderq\\xe9KzU\\x041981q\\xeaK{U\\x042002q\\xebK|U\\x05tampaq\\xecK}U\\x03bayq\\xedK~U\\x05devilq\\xeeK\\x7fU\\x04raysq\\xefK\\x80U\\x04runsq\\xf0K\\x81U\\x07triplesq\\xf1K\\x82U\\x0222q\\xf2K\\x83U\\x041960q\\xf3K\\x84U\\x04punkq\\xf4K\\x85U\\x06carterq\\xf5K\\x86U\\x03usmq\\xf6K\\x87U\\x05jamesq\\xf7K\\x88U\\x08morrisonq\\xf8K\\x89U\\x041972q\\xf9K\\x8aU\\x07emporiaq\\xfaK\\x8bU\\x08virginiaq\\xfbK\\x8cU\\tdefensiveq\\xfcK\\x8dU\\x07linemanq\\xfdK\\x8eU\\x05diegoq\\xfeK\\x8fU\\x08chargersq\\xffK\\x90U\\x07seattler\\x00\\x01\\x00\\x00K\\x91U\\x08seahawksr\\x01\\x01\\x00\\x00K\\x92U\\x03newr\\x02\\x01\\x00\\x00K\\x93U\\x08patriotsr\\x03\\x01\\x00\\x00K\\x94U\\tbaltimorer\\x04\\x01\\x00\\x00K\\x95U\\x06ravensr\\x05\\x01\\x00\\x00K\\x96U\\tfranciscor\\x06\\x01\\x00\\x00K\\x97U\\x0549ersr\\x07\\x01\\x00\\x00K\\x98U\\x07riddickr\\x08\\x01\\x00\\x00K\\x99U\\x06parkerr\\t\\x01\\x00\\x00K\\x9aU\\x0227r\\n\\x01\\x00\\x00K\\x9bU\\x08catholicr\\x0b\\x01\\x00\\x00K\\x9cU\\tanchoressr\\x0c\\x01\\x00\\x00K\\x9dU\\x041934r\\r\\x01\\x00\\x00K\\x9eU\\tbeatifiedr\\x0e\\x01\\x00\\x00K\\x9fU\\nmontenegror\\x0f\\x01\\x00\\x00K\\xa0U\\x05khailr\\x10\\x01\\x00\\x00K\\xa1U\\x05laterr\\x11\\x01\\x00\\x00K\\xa2U\\x05worldr\\x12\\x01\\x00\\x00K\\xa3U\\x02anr\\x13\\x01\\x00\\x00K\\xa4U\\x04bestr\\x14\\x01\\x00\\x00K\\xa5U\\x08footballr\\x15\\x01\\x00\\x00K\\xa6U\\x0456thr\\x16\\x01\\x00\\x00K\\xa7U\\x01&K\\xa8U\\x08shenoudar\\x17\\x01\\x00\\x00K\\xa9U\\x07gabrielr\\x18\\x01\\x00\\x00K\\xaaU\\tmonasteryr\\x19\\x01\\x00\\x00K\\xabU\\x08macariusr\\x1a\\x01\\x00\\x00K\\xacU\\x05greatr\\x1b\\x01\\x00\\x00K\\xadU\\x08egyptianr\\x1c\\x01\\x00\\x00K\\xaeU\\x08orthodoxr\\x1d\\x01\\x00\\x00K\\xafU\\tchristianr\\x1e\\x01\\x00\\x00K\\xb0U\\x08baramhatr\\x1f\\x01\\x00\\x00K\\xb1U\\x08calendarr \\x01\\x00\\x00K\\xb2U\\x041.85r!\\x01\\x00\\x00K\\xb3U\\x08merzifonr\"\\x01\\x00\\x00K\\xb4U\\x06turkeyr#\\x01\\x00\\x00K\\xb5U\\x0215r$\\x01\\x00\\x00K\\xb6U\\x0cmerzifonsporr%\\x01\\x00\\x00K\\xb7U\\x06p.t.t.r&\\x01\\x00\\x00K\\xb8U\\x05altayr\\'\\x01\\x00\\x00K\\xb9U\\x03\\xe2\\x86\\x92r(\\x01\\x00\\x00K\\xbaU\\x06batmanr)\\x01\\x00\\x00K\\xbbU\\npetrolsporr*\\x01\\x00\\x00K\\xbcU\\x04loanr+\\x01\\x00\\x00K\\xbdU\\tsiirtsporr,\\x01\\x00\\x00K\\xbeU\\x10gen\\xc3\\xa7lerbirli\\xc4\\x9fir-\\x01\\x00\\x00K\\xbfU\\x0bkar\\xc5\\x9f\\xc4\\xb1yakar.\\x01\\x00\\x00K\\xc0U\\rboz\\xc3\\xbcy\\xc3\\xbcksporr/\\x01\\x00\\x00K\\xc1U\\x0224r0\\x01\\x00\\x00K\\xc2U\\x0252r1\\x01\\x00\\x00K\\xc3U\\x0233r2\\x01\\x00\\x00K\\xc4U\\x0264r3\\x01\\x00\\x00K\\xc5U\\x03160r4\\x01\\x00\\x00K\\xc6U\\x0261r5\\x01\\x00\\x00K\\xc7U\\x0226r6\\x01\\x00\\x00K\\xc8U\\x0258r7\\x01\\x00\\x00K\\xc9U\\x0223r8\\x01\\x00\\x00K\\xcaU\\x0242r9\\x01\\x00\\x00K\\xcbU\\x0219r:\\x01\\x00\\x00K\\xccU\\x0211r;\\x01\\x00\\x00K\\xcdU\\x014K\\xceU\\x0206r<\\x01\\x00\\x00K\\xcfU\\x07januaryr=\\x01\\x00\\x00K\\xd0U\\x042013r>\\x01\\x00\\x00K\\xd1U\\x03utcr?\\x01\\x00\\x00K\\xd2U\\x06n\\xc3\\xaemesr@\\x01\\x00\\x00K\\xd3U\\x05rightrA\\x01\\x00\\x00K\\xd4U\\x06handedrB\\x01\\x00\\x00K\\xd5U\\x04jackrC\\x01\\x00\\x00K\\xd6U\\x05wyantrD\\x01\\x00\\x00K\\xd7U\\x05gillyrE\\x01\\x00\\x00K\\xd8U\\x04lanerF\\x01\\x00\\x00K\\xd9U\\x07richardrG\\x01\\x00\\x00K\\xdaU\\x04doddrH\\x01\\x00\\x00K\\xdbU\\ntecnifibrerI\\x01\\x00\\x00K\\xdcU\\x05womenrJ\\x01\\x00\\x00K\\xddU\\x07singlesrK\\x01\\x00\\x00K\\xdeU\\x04junerL\\x01\\x00\\x00K\\xdfU\\x0421strM\\x01\\x00\\x00K\\xe0U\\x041969rN\\x01\\x00\\x00K\\xe1U\\x041974rO\\x01\\x00\\x00K\\xe2U\\x08districtrP\\x01\\x00\\x00K\\xe3U\\x07createdrQ\\x01\\x00\\x00K\\xe4U\\x06thomasrR\\x01\\x00\\x00K\\xe5U\\x02e.rS\\x01\\x00\\x00K\\xe6U\\x08flahertyrT\\x01\\x00\\x00K\\xe7U\\x041940rU\\x01\\x00\\x00K\\xe8U\\x08duquesnerV\\x01\\x00\\x00K\\xe9U\\ngeorgetownrW\\x01\\x00\\x00K\\xeaU\\x06marionrX\\x01\\x00\\x00K\\xebU\\x05masonrY\\x01\\x00\\x00K\\xecU\\x02m.rZ\\x01\\x00\\x00K\\xedU\\x041955r[\\x01\\x00\\x00K\\xeeU\\x081962.jpgr\\\\\\x01\\x00\\x00K\\xefU\\x041962r]\\x01\\x00\\x00K\\xf0U\\tmilwaukeer^\\x01\\x00\\x00K\\xf1U\\twisconsinr_\\x01\\x00\\x00K\\xf2U\\x04u.s.r`\\x01\\x00\\x00K\\xf3U\\x041956ra\\x01\\x00\\x00K\\xf4U\\x07presentrb\\x01\\x00\\x00K\\xf5U\\x082013.jpgrc\\x01\\x00\\x00K\\xf6U\\x05250pxrd\\x01\\x00\\x00K\\xf7U\\x07houstonre\\x01\\x00\\x00K\\xf8U\\x05texasrf\\x01\\x00\\x00K\\xf9U\\x07battingrg\\x01\\x00\\x00K\\xfaU\\x07averagerh\\x01\\x00\\x00K\\xfbU\\x04hitsri\\x01\\x00\\x00K\\xfcU\\x04homerj\\x01\\x00\\x00K\\xfdU\\x06battedrk\\x01\\x00\\x00K\\xfeU\\x06stolenrl\\x01\\x00\\x00K\\xffU\\x05basesrm\\x01\\x00\\x00M\\x00\\x01U\\x04.284rn\\x01\\x00\\x00M\\x01\\x01U\\x051,893ro\\x01\\x00\\x00M\\x02\\x01U\\x12134\\xc2\\xa0754\\xc2\\xa0473\\xc2\\xa0121rp\\x01\\x00\\x00M\\x03\\x01U\\nrelentlessrq\\x01\\x00\\x00M\\x04\\x01U\\ngarage.jpgrr\\x01\\x00\\x00M\\x05\\x01U\\nperformingrs\\x01\\x00\\x00M\\x06\\x01U\\x06garagert\\x01\\x00\\x00M\\x07\\x01U\\x042010ru\\x01\\x00\\x00M\\x08\\x01U\\x0bsolo_singerrv\\x01\\x00\\x00M\\t\\x01U\\x06londonrw\\x01\\x00\\x00M\\n\\x01U\\x04rockrx\\x01\\x00\\x00M\\x0b\\x01U\\x08acousticry\\x01\\x00\\x00M\\x0c\\x01U\\x041985rz\\x01\\x00\\x00M\\r\\x01U\\x03tenr{\\x01\\x00\\x00M\\x0e\\x01U\\x05fortyr|\\x01\\x00\\x00M\\x0f\\x01U\\x05soundr}\\x01\\x00\\x00M\\x10\\x01U\\x06cherryr~\\x01\\x00\\x00M\\x11\\x01U\\x03redr\\x7f\\x01\\x00\\x00M\\x12\\x01U\\x03emir\\x80\\x01\\x00\\x00M\\x13\\x01U\\x03bigr\\x81\\x01\\x00\\x00M\\x14\\x01U\\x03catr\\x82\\x01\\x00\\x00M\\x15\\x01U\\x05roughr\\x83\\x01\\x00\\x00M\\x16\\x01U\\x05trader\\x84\\x01\\x00\\x00M\\x17\\x01U\\x06fiercer\\x85\\x01\\x00\\x00M\\x18\\x01U\\x05pandar\\x86\\x01\\x00\\x00M\\x19\\x01U\\x05superr\\x87\\x01\\x00\\x00M\\x1a\\x01U\\x0bstereoworldr\\x88\\x01\\x00\\x00M\\x1b\\x01U\\x06robertr\\x89\\x01\\x00\\x00M\\x1c\\x01U\\x0cabdoujaparovr\\x8a\\x01\\x00\\x00M\\x1d\\x01U\\x04idour\\x8b\\x01\\x00\\x00M\\x1e\\x01U\\x05chrisr\\x8c\\x01\\x00\\x00M\\x1f\\x01U\\x03t-tr\\x8d\\x01\\x00\\x00M \\x01U\\x0297r\\x8e\\x01\\x00\\x00M!\\x01U\\x05northr\\x8f\\x01\\x00\\x00M\"\\x01U\\x08carolinar\\x90\\x01\\x00\\x00M#\\x01U\\x016M$\\x01U\\x03295r\\x91\\x01\\x00\\x00M%\\x01U\\x01yM&\\x01U\\nparkerid01r\\x92\\x01\\x00\\x00M\\'\\x01U\\x151995\\xc2\\xa01996-2000\\xc2\\xa02001r\\x93\\x01\\x00\\x00M(\\x01U\\x0f2002-2003\\xc2\\xa02004r\\x94\\x01\\x00\\x00M)\\x01U\\x05ozanar\\x95\\x01\\x00\\x00M*\\x01U\\x08kotorskar\\x96\\x01\\x00\\x00M+\\x01U\\x041493r\\x97\\x01\\x00\\x00M,\\x01U\\x041565r\\x98\\x01\\x00\\x00M-\\x01U\\x05romanr\\x99\\x01\\x00\\x00M.\\x01U\\x10ozanaofkotor.jpgr\\x9a\\x01\\x00\\x00M/\\x01U\\x05129pxr\\x9b\\x01\\x00\\x00M0\\x01U\\x06relezir\\x9c\\x01\\x00\\x00M1\\x01U\\x02orr\\x9d\\x01\\x00\\x00M2\\x01U\\x06kumanor\\x9e\\x01\\x00\\x00M3\\x01U\\x0cprincipalityr\\x9f\\x01\\x00\\x00M4\\x01U\\x0fzeta/montenegror\\xa0\\x01\\x00\\x00M5\\x01U\\x07albaniar\\xa1\\x01\\x00\\x00M6\\x01U\\x06venetar\\xa2\\x01\\x00\\x00M7\\x01U\\x08republicr\\xa3\\x01\\x00\\x00M8\\x01U\\x06venicer\\xa4\\x01\\x00\\x00M9\\x01U\\x06virginr\\xa5\\x01\\x00\\x00M:\\x01U\\x06mysticr\\xa6\\x01\\x00\\x00M;\\x01U\\x041928r\\xa7\\x01\\x00\\x00M<\\x01U\\x06cultusr\\xa8\\x01\\x00\\x00M=\\x01U\\tconfirmedr\\xa9\\x01\\x00\\x00M>\\x01U\\x04piusr\\xaa\\x01\\x00\\x00M?\\x01U\\x02xir\\xab\\x01\\x00\\x00M@\\x01U\\x04alsor\\xac\\x01\\x00\\x00MA\\x01U\\x03882r\\xad\\x01\\x00\\x00MB\\x01U\\x08governorr\\xae\\x01\\x00\\x00MC\\x01U\\x05ahmadr\\xaf\\x01\\x00\\x00MD\\x01U\\x03ibnr\\xb0\\x01\\x00\\x00ME\\x01U\\x05tulunr\\xb1\\x01\\x00\\x00MF\\x01U\\x06forcedr\\xb2\\x01\\x00\\x00MG\\x01U\\x03payr\\xb3\\x01\\x00\\x00MH\\x01U\\x05heavyr\\xb4\\x01\\x00\\x00MI\\x01U\\rcontributionsr\\xb5\\x01\\x00\\x00MJ\\x01U\\x07forcingr\\xb6\\x01\\x00\\x00MK\\x01U\\x03himr\\xb7\\x01\\x00\\x00ML\\x01U\\x04sellr\\xb8\\x01\\x00\\x00MM\\x01U\\x04somer\\xb9\\x01\\x00\\x00MN\\x01U\\x08attachedr\\xba\\x01\\x00\\x00MO\\x01U\\npropertiesr\\xbb\\x01\\x00\\x00MP\\x01U\\x05localr\\xbc\\x01\\x00\\x00MQ\\x01U\\x06jewishr\\xbd\\x01\\x00\\x00MR\\x01U\\tcommunityr\\xbe\\x01\\x00\\x00MS\\x01U\\x04thisr\\xbf\\x01\\x00\\x00MT\\x01U\\x08buildingr\\xc0\\x01\\x00\\x00MU\\x01U\\x03oner\\xc1\\x01\\x00\\x00MV\\x01U\\x04timer\\xc2\\x01\\x00\\x00MW\\x01U\\x08believedr\\xc3\\x01\\x00\\x00MX\\x01U\\x04haver\\xc4\\x01\\x00\\x00MY\\x01U\\x06becomer\\xc5\\x01\\x00\\x00MZ\\x01U\\x04siter\\xc6\\x01\\x00\\x00M[\\x01U\\x05cairor\\xc7\\x01\\x00\\x00M\\\\\\x01U\\x06genizar\\xc8\\x01\\x00\\x00M]\\x01U\\x04maler\\xc9\\x01\\x00\\x00M^\\x01U\\x05tabler\\xca\\x01\\x00\\x00M_\\x01U\\x06tennisr\\xcb\\x01\\x00\\x00M`\\x01U\\x05chinar\\xcc\\x01\\x00\\x00Ma\\x01U\\x07turkishr\\xcd\\x01\\x00\\x00Mb\\x01U\\nfootballerr\\xce\\x01\\x00\\x00Mc\\x01U\\tcurrentlyr\\xcf\\x01\\x00\\x00Md\\x01U\\x05playsr\\xd0\\x01\\x00\\x00Me\\x01U\\x06squashr\\xd1\\x01\\x00\\x00Mf\\x01U\\x03whor\\xd2\\x01\\x00\\x00Mg\\x01U\\nrepresentsr\\xd3\\x01\\x00\\x00Mh\\x01U\\x07reachedr\\xd4\\x01\\x00\\x00Mi\\x01U\\x0bcareer-highr\\xd5\\x01\\x00\\x00Mj\\x01U\\x07rankingr\\xd6\\x01\\x00\\x00Mk\\x01U\\x06memberr\\xd7\\x01\\x00\\x00Ml\\x01U\\x05houser\\xd8\\x01\\x00\\x00Mm\\x01U\\x0frepresentativesr\\xd9\\x01\\x00\\x00Mn\\x01U\\x06angelar\\xda\\x01\\x00\\x00Mo\\x01U\\x08pitullior\\xdb\\x01\\x00\\x00Mp\\x01U\\x05stager\\xdc\\x01\\x00\\x00Mq\\x01U\\x04filmr\\xdd\\x01\\x00\\x00Mr\\x01U\\ntelevisionr\\xde\\x01\\x00\\x00Ms\\x01U\\x07perhapsr\\xdf\\x01\\x00\\x00Mt\\x01U\\nportrayingr\\xe0\\x01\\x00\\x00Mu\\x01U\\x06femaler\\xe1\\x01\\x00\\x00Mv\\x01U\\nchangelingr\\xe2\\x01\\x00\\x00Mw\\x01U\\x07demonter\\xe3\\x01\\x00\\x00Mx\\x01U\\tnicknamedr\\xe4\\x01\\x00\\x00My\\x01U\\x02``r\\xe5\\x01\\x00\\x00Mz\\x01U\\x07perfectr\\xe6\\x01\\x00\\x00M{\\x01U\\x05stormr\\xe7\\x01\\x00\\x00M|\\x01U\\x03mlbr\\xe8\\x01\\x00\\x00M}\\x01U\\x04batsr\\xe9\\x01\\x00\\x00M~\\x01U\\x06throwsr\\xea\\x01\\x00\\x00M\\x7f\\x01U\\x0bleft-handedr\\xeb\\x01\\x00\\x00M\\x80\\x01U\\x07draftedr\\xec\\x01\\x00\\x00M\\x81\\x01U\\x02byr\\xed\\x01\\x00\\x00M\\x82\\x01U\\x06secondr\\xee\\x01\\x00\\x00M\\x83\\x01U\\x05roundr\\xef\\x01\\x00\\x00M\\x84\\x01U\\x0452ndr\\xf0\\x01\\x00\\x00M\\x85\\x01U\\x07overallr\\xf1\\x01\\x00\\x00M\\x86\\x01U\\x05draftr\\xf2\\x01\\x00\\x00M\\x87\\x01U\\x04mader\\xf3\\x01\\x00\\x00M\\x88\\x01U\\x03hisr\\xf4\\x01\\x00\\x00M\\x89\\x01U\\x05debutr\\xf5\\x01\\x00\\x00M\\x8a\\x01U\\x03hasr\\xf6\\x01\\x00\\x00M\\x8b\\x01U\\x04morer\\xf7\\x01\\x00\\x00M\\x8c\\x01U\\x03121r\\xf8\\x01\\x00\\x00M\\x8d\\x01U\\x04thanr\\xf9\\x01\\x00\\x00M\\x8e\\x01U\\x03anyr\\xfa\\x01\\x00\\x00M\\x8f\\x01U\\x05otherr\\xfb\\x01\\x00\\x00M\\x90\\x01U\\x04neilr\\xfc\\x01\\x00\\x00M\\x91\\x01U\\x07britishr\\xfd\\x01\\x00\\x00M\\x92\\x01U\\x08musicianr\\xfe\\x01\\x00\\x00M\\x93\\x01U\\x06authorr\\xff\\x01\\x00\\x00M\\x94\\x01U\\x06singerr\\x00\\x02\\x00\\x00M\\x95\\x01U\\x05indier\\x01\\x02\\x00\\x00M\\x96\\x01U\\x04bandr\\x02\\x02\\x00\\x00M\\x97\\x01U\\x08nationalr\\x03\\x02\\x00\\x00M\\x98\\x01U\\x08t.o.s.d.r\\x04\\x02\\x00\\x00M\\x99\\x01U\\tvisionaryr\\x05\\x02\\x00\\x00M\\x9a\\x01U\\x07teenager\\x06\\x02\\x00\\x00M\\x9b\\x01U\\x07convertr\\x07\\x02\\x00\\x00M\\x9c\\x01U\\torthodoxyr\\x08\\x02\\x00\\x00M\\x9d\\x01U\\x07serbianr\\t\\x02\\x00\\x00M\\x9e\\x01U\\x07descentr\\n\\x02\\x00\\x00M\\x9f\\x01U\\x04zetar\\x0b\\x02\\x00\\x00M\\xa0\\x01U\\x06becamer\\x0c\\x02\\x00\\x00M\\xa1\\x01U\\tdominicanr\\r\\x02\\x00\\x00M\\xa2\\x01U\\x08tertiaryr\\x0e\\x02\\x00\\x00M\\xa3\\x01U\\x0cposthumouslyr\\x0f\\x02\\x00\\x00M\\xa4\\x01U\\tveneratedr\\x10\\x02\\x00\\x00uU\\x1crev_content_union_label_dict}r\\x11\\x02\\x00\\x00(j\\xe0\\x01\\x00\\x00Mt\\x01j\\xc8\\x01\\x00\\x00M\\\\\\x01jX\\x01\\x00\\x00K\\xeaj\\xb5\\x01\\x00\\x00MI\\x01jM\\x01\\x00\\x00K\\xdfj\\x10\\x02\\x00\\x00M\\xa4\\x01jT\\x01\\x00\\x00K\\xe6j\\xcd\\x01\\x00\\x00Ma\\x01jU\\x01\\x00\\x00K\\xe7U\\x010Kmj\\x8e\\x01\\x00\\x00M \\x01j}\\x01\\x00\\x00M\\x0f\\x01j\\x1c\\x01\\x00\\x00K\\xadj\\x93\\x01\\x00\\x00M\\'\\x01j\\x97\\x01\\x00\\x00M+\\x01jC\\x01\\x00\\x00K\\xd5h\\xb3K=h\\x7fK\\x08h\\x9cK&h\\x95K\\x1fj\\xbf\\x01\\x00\\x00MS\\x01j\\xd5\\x01\\x00\\x00Mi\\x01j\\x7f\\x01\\x00\\x00M\\x11\\x01j\\x18\\x01\\x00\\x00K\\xa9j\\x91\\x01\\x00\\x00M$\\x01j\\xef\\x01\\x00\\x00M\\x83\\x01jV\\x01\\x00\\x00K\\xe8h\\xf5K\\x85j{\\x01\\x00\\x00M\\r\\x01j\\x08\\x01\\x00\\x00K\\x98j\\xee\\x01\\x00\\x00M\\x82\\x01h\\xc0KNj\\x86\\x01\\x00\\x00M\\x18\\x01j\\xa5\\x01\\x00\\x00M9\\x01j\\xd0\\x01\\x00\\x00Md\\x01jG\\x01\\x00\\x00K\\xd9j\\x94\\x01\\x00\\x00M(\\x01j\\x02\\x01\\x00\\x00K\\x92j\\xec\\x01\\x00\\x00M\\x80\\x01j\\x1e\\x01\\x00\\x00K\\xafj\\xa4\\x01\\x00\\x00M8\\x01h\\xaaK4h\\xddKlh\\xdfKoh\\x86K\\x10h\\xbaKGj\\x08\\x02\\x00\\x00M\\x9c\\x01j\\x07\\x02\\x00\\x00M\\x9b\\x01j\\xc9\\x01\\x00\\x00M]\\x01h\\xadK7j2\\x01\\x00\\x00K\\xc3h\\xd2Kah\\x92K\\x1cj\\x06\\x02\\x00\\x00M\\x9a\\x01h\\x8dK\\x17h\\xe5Kuh\\xd1K`j\\x03\\x01\\x00\\x00K\\x93j\\x0e\\x02\\x00\\x00M\\xa2\\x01h\\xa7K1h\\x93K\\x1dh\\xf6K\\x86jZ\\x01\\x00\\x00K\\xecj\\xda\\x01\\x00\\x00Mn\\x01h\\xb8KEh\\xffK\\x8fU\\x016M#\\x01h\\xe1Kqj\\xf7\\x01\\x00\\x00M\\x8b\\x01j\\'\\x01\\x00\\x00K\\xb8j\\xfe\\x01\\x00\\x00M\\x92\\x01h\\xbbKIh\\xc5KSh\\x96K h\\xecK|jJ\\x01\\x00\\x00K\\xdch\\xd5Kdj\\xb0\\x01\\x00\\x00MD\\x01jH\\x01\\x00\\x00K\\xdah\\x9dK\\'h\\xb4K>h\\xccKZj\\x82\\x01\\x00\\x00M\\x14\\x01j(\\x01\\x00\\x00K\\xb9j\\xc3\\x01\\x00\\x00MW\\x01jv\\x01\\x00\\x00M\\x08\\x01j\\x0e\\x01\\x00\\x00K\\x9ejc\\x01\\x00\\x00K\\xf5j\\x01\\x02\\x00\\x00M\\x95\\x01h\\xefK\\x7fh\\xc9KWU\\x011KHh\\xe4Kth\\x9aK$U\\x01aK\\th\\x9fK)je\\x01\\x00\\x00K\\xf7h\\xb9KFj\\x84\\x01\\x00\\x00M\\x16\\x01jw\\x01\\x00\\x00M\\t\\x01j^\\x01\\x00\\x00K\\xf0j0\\x01\\x00\\x00K\\xc1h\\xcfK]j6\\x01\\x00\\x00K\\xc7j\\n\\x01\\x00\\x00K\\x9ah\\x90K\\x1ah\\xe7Kwh\\xf2K\\x82j8\\x01\\x00\\x00K\\xc9h\\x8bK\\x15U\\x01,K\\x02h\\x8aK\\x14h\\xdaKiju\\x01\\x00\\x00M\\x07\\x01j>\\x01\\x00\\x00K\\xd0h\\xdbKjj\\x1d\\x01\\x00\\x00K\\xaej[\\x01\\x00\\x00K\\xedj\\x02\\x02\\x00\\x00M\\x96\\x01ja\\x01\\x00\\x00K\\xf3j\\xd3\\x01\\x00\\x00Mg\\x01h\\xe0Kph\\xf7K\\x87h\\xe3Ksj\\xf5\\x01\\x00\\x00M\\x89\\x01j\\x88\\x01\\x00\\x00M\\x1a\\x01jx\\x01\\x00\\x00M\\n\\x01jy\\x01\\x00\\x00M\\x0b\\x01j\\xb4\\x01\\x00\\x00MH\\x01h\\xc3KQj\\xc7\\x01\\x00\\x00M[\\x01j/\\x01\\x00\\x00K\\xc0j\\x89\\x01\\x00\\x00M\\x1b\\x01j|\\x01\\x00\\x00M\\x0e\\x01j\\x17\\x01\\x00\\x00K\\xa8jY\\x01\\x00\\x00K\\xebhzK\\x01jS\\x01\\x00\\x00K\\xe5j_\\x01\\x00\\x00K\\xf1j`\\x01\\x00\\x00K\\xf2h\\xa1K+jD\\x01\\x00\\x00K\\xd6jQ\\x01\\x00\\x00K\\xe3h\\xf4K\\x84j\\xfd\\x01\\x00\\x00M\\x91\\x01j\\x07\\x01\\x00\\x00K\\x97j\\xb1\\x01\\x00\\x00ME\\x01j\\xea\\x01\\x00\\x00M~\\x01h\\xcbKYj\\xab\\x01\\x00\\x00M?\\x01j\\x9b\\x01\\x00\\x00M/\\x01j\\xc1\\x01\\x00\\x00MU\\x01j\\x04\\x02\\x00\\x00M\\x98\\x01j\\xd4\\x01\\x00\\x00Mh\\x01j\\xbb\\x01\\x00\\x00MO\\x01h\\xfaK\\x8ajP\\x01\\x00\\x00K\\xe2h\\xbeKLh\\xceK\\\\j%\\x01\\x00\\x00K\\xb6h\\xb7KDh\\xa4K.h\\xaeK8j\\xf2\\x01\\x00\\x00M\\x86\\x01jW\\x01\\x00\\x00K\\xe9j\\xe7\\x01\\x00\\x00M{\\x01j\\x0f\\x01\\x00\\x00K\\x9fh\\x98K\"j.\\x01\\x00\\x00K\\xbfh\\x9eK(j\\x10\\x01\\x00\\x00K\\xa0j\\xf9\\x01\\x00\\x00M\\x8d\\x01j;\\x01\\x00\\x00K\\xcch\\xb2K<j\\xde\\x01\\x00\\x00Mr\\x01h\\xdeKnj$\\x01\\x00\\x00K\\xb5h\\xb5KBj\\x9a\\x01\\x00\\x00M.\\x01h\\x97K!j:\\x01\\x00\\x00K\\xcbh\\xd3Kbj\\x0f\\x02\\x00\\x00M\\xa3\\x01h\\x81K\\x0bj\\xad\\x01\\x00\\x00MA\\x01h\\xd0K^h\\xa3K-j\\xeb\\x01\\x00\\x00M\\x7f\\x01j\\xc4\\x01\\x00\\x00MX\\x01j\\xfa\\x01\\x00\\x00M\\x8e\\x01jg\\x01\\x00\\x00K\\xf9j\\xb8\\x01\\x00\\x00ML\\x01j\\xb2\\x01\\x00\\x00MF\\x01jN\\x01\\x00\\x00K\\xe0h\\xf3K\\x83j]\\x01\\x00\\x00K\\xefj\\xac\\x01\\x00\\x00M@\\x01jK\\x01\\x00\\x00K\\xddj\\x19\\x01\\x00\\x00K\\xaaj\\xd2\\x01\\x00\\x00Mf\\x01jo\\x01\\x00\\x00M\\x01\\x01h\\xb6KCj\\x00\\x02\\x00\\x00M\\x94\\x01h\\xe2Krjh\\x01\\x00\\x00K\\xfaj\\x11\\x01\\x00\\x00K\\xa1h\\xa8K2jR\\x01\\x00\\x00K\\xe4j\\t\\x01\\x00\\x00K\\x99h\\x89K\\x13j\\x9e\\x01\\x00\\x00M2\\x01j\\xa0\\x01\\x00\\x00M4\\x01j\\x83\\x01\\x00\\x00M\\x15\\x01j\\x9f\\x01\\x00\\x00M3\\x01j\\xdb\\x01\\x00\\x00Mo\\x01jE\\x01\\x00\\x00K\\xd7jm\\x01\\x00\\x00K\\xffU\\x018K?j\\xbc\\x01\\x00\\x00MP\\x01j\\x90\\x01\\x00\\x00M\"\\x01j&\\x01\\x00\\x00K\\xb7j\\xd6\\x01\\x00\\x00Mj\\x01jk\\x01\\x00\\x00K\\xfdj\\xf4\\x01\\x00\\x00M\\x88\\x01j\\xf1\\x01\\x00\\x00M\\x85\\x01h\\x83K\\rj\\xf8\\x01\\x00\\x00M\\x8c\\x01j!\\x01\\x00\\x00K\\xb2j\\xb7\\x01\\x00\\x00MK\\x01h\\xf0K\\x80j\\x80\\x01\\x00\\x00M\\x12\\x01j\\xaf\\x01\\x00\\x00MC\\x01h\\xedK}h\\x94K\\x1eh\\xafK9jq\\x01\\x00\\x00M\\x03\\x01j\\xa1\\x01\\x00\\x00M5\\x01j\\x03\\x02\\x00\\x00M\\x97\\x01h\\xcdK[j\\xa7\\x01\\x00\\x00M;\\x01j \\x01\\x00\\x00K\\xb1j\\x14\\x01\\x00\\x00K\\xa4j<\\x01\\x00\\x00K\\xcej\\xcf\\x01\\x00\\x00Mc\\x01j@\\x01\\x00\\x00K\\xd2j\\xd9\\x01\\x00\\x00Mm\\x01U\\x013KAj\\xe6\\x01\\x00\\x00Mz\\x01h\\xf1K\\x81h\\xc1KOjd\\x01\\x00\\x00K\\xf6h\\xabK5ji\\x01\\x00\\x00K\\xfbj\\x92\\x01\\x00\\x00M&\\x01h\\x8eK\\x18j\\x99\\x01\\x00\\x00M-\\x01j\\x0c\\x02\\x00\\x00M\\xa0\\x01j\\xcc\\x01\\x00\\x00M`\\x01j\\x04\\x01\\x00\\x00K\\x94j+\\x01\\x00\\x00K\\xbcj\\xbe\\x01\\x00\\x00MR\\x01j\\xaa\\x01\\x00\\x00M>\\x01j5\\x01\\x00\\x00K\\xc6j3\\x01\\x00\\x00K\\xc4h\\xb1K;j\\xca\\x01\\x00\\x00M^\\x01j\\x0b\\x02\\x00\\x00M\\x9f\\x01j#\\x01\\x00\\x00K\\xb4h\\xeeK~j\\x8b\\x01\\x00\\x00M\\x1d\\x01U\\x01.K\\x06h\\x87K\\x11j\\x1a\\x01\\x00\\x00K\\xabj\\x1b\\x01\\x00\\x00K\\xacj\\x05\\x01\\x00\\x00K\\x95j4\\x01\\x00\\x00K\\xc5h\\xc6KTjb\\x01\\x00\\x00K\\xf4j\\xe8\\x01\\x00\\x00M|\\x01jl\\x01\\x00\\x00K\\xfej\\xae\\x01\\x00\\x00MB\\x01jf\\x01\\x00\\x00K\\xf8jI\\x01\\x00\\x00K\\xdbj\\x95\\x01\\x00\\x00M)\\x01j\\xa6\\x01\\x00\\x00M:\\x01h\\x84K\\x0ej\\xc6\\x01\\x00\\x00MZ\\x01h\\xc7KUj\\x9c\\x01\\x00\\x00M0\\x01h\\x80K\\nj\"\\x01\\x00\\x00K\\xb3j\\x00\\x01\\x00\\x00K\\x90h\\xdcKkj\\xa9\\x01\\x00\\x00M=\\x01j\\n\\x02\\x00\\x00M\\x9e\\x01j\\xff\\x01\\x00\\x00M\\x93\\x01j\\xdf\\x01\\x00\\x00Ms\\x01j\\xb3\\x01\\x00\\x00MG\\x01j\\xd7\\x01\\x00\\x00Mk\\x01j\\x0b\\x01\\x00\\x00K\\x9bj\\x85\\x01\\x00\\x00M\\x17\\x01h\\xe9Kyh\\xbcKJjO\\x01\\x00\\x00K\\xe1h\\xf9K\\x89U\\x01yM%\\x01h~K\\x07h\\xa2K,h\\xe6Kvj\\xe4\\x01\\x00\\x00Mx\\x01h\\xfdK\\x8dj~\\x01\\x00\\x00M\\x10\\x01U\\x014K\\xcdU\\x015K@j\\xf6\\x01\\x00\\x00M\\x8a\\x01j\\xce\\x01\\x00\\x00Mb\\x01h\\x8fK\\x19j\\x81\\x01\\x00\\x00M\\x13\\x01h\\xfbK\\x8bj\\x01\\x01\\x00\\x00K\\x91j\\x12\\x01\\x00\\x00K\\xa2j7\\x01\\x00\\x00K\\xc8js\\x01\\x00\\x00M\\x05\\x01j1\\x01\\x00\\x00K\\xc2jt\\x01\\x00\\x00M\\x06\\x01j=\\x01\\x00\\x00K\\xcfj\\xc5\\x01\\x00\\x00MY\\x01jn\\x01\\x00\\x00M\\x00\\x01j\\x16\\x01\\x00\\x00K\\xa6jA\\x01\\x00\\x00K\\xd3j\\xb9\\x01\\x00\\x00MM\\x01j\\xe2\\x01\\x00\\x00Mv\\x01h\\x88K\\x12jj\\x01\\x00\\x00K\\xfcj\\xf0\\x01\\x00\\x00M\\x84\\x01h\\xc8KVjp\\x01\\x00\\x00M\\x02\\x01jr\\x01\\x00\\x00M\\x04\\x01j\\xa2\\x01\\x00\\x00M6\\x01h\\x82K\\x0cj\\x05\\x02\\x00\\x00M\\x99\\x01j\\t\\x02\\x00\\x00M\\x9d\\x01h\\xfcK\\x8cj\\r\\x01\\x00\\x00K\\x9dj,\\x01\\x00\\x00K\\xbdj\\x87\\x01\\x00\\x00M\\x19\\x01j\\xed\\x01\\x00\\x00M\\x81\\x01j\\xdc\\x01\\x00\\x00Mp\\x01h\\xbdKKj*\\x01\\x00\\x00K\\xbbh{K\\x03U\\x01&K\\xa7h\\xbfKMj\\x9d\\x01\\x00\\x00M1\\x01h\\x91K\\x1bh\\x8cK\\x16h\\xc4KRj\\xe9\\x01\\x00\\x00M}\\x01j\\xe1\\x01\\x00\\x00Mu\\x01j\\x0c\\x01\\x00\\x00K\\x9cj\\xfc\\x01\\x00\\x00M\\x90\\x01h\\xacK6jz\\x01\\x00\\x00M\\x0c\\x01U\\x01iK_h\\xeaKzj\\xcb\\x01\\x00\\x00M_\\x01j\\xbd\\x01\\x00\\x00MQ\\x01h\\x9bK%j\\xd8\\x01\\x00\\x00Ml\\x01h\\x85K\\x0fh\\xf8K\\x88j-\\x01\\x00\\x00K\\xbej\\x8f\\x01\\x00\\x00M!\\x01h\\xa5K/j)\\x01\\x00\\x00K\\xbah\\x99K#h\\xd4Kcj\\\\\\x01\\x00\\x00K\\xeeh|K\\x04h\\xe8Kxh\\xa9K3j?\\x01\\x00\\x00K\\xd1j\\xf3\\x01\\x00\\x00M\\x87\\x01j\\r\\x02\\x00\\x00M\\xa1\\x01j\\xba\\x01\\x00\\x00MN\\x01h\\xa0K*h\\x04K\\x00h}K\\x05j\\x96\\x01\\x00\\x00M*\\x01j\\xe5\\x01\\x00\\x00My\\x01j\\x8a\\x01\\x00\\x00M\\x1c\\x01j\\x13\\x01\\x00\\x00K\\xa3h\\xcaKXh\\xa6K0h\\xc2KPj\\xa8\\x01\\x00\\x00M<\\x01j\\xdd\\x01\\x00\\x00Mq\\x01h\\xfeK\\x8ej9\\x01\\x00\\x00K\\xcajB\\x01\\x00\\x00K\\xd4j\\xe3\\x01\\x00\\x00Mw\\x01j\\xfb\\x01\\x00\\x00M\\x8f\\x01j\\xa3\\x01\\x00\\x00M7\\x01j\\xb6\\x01\\x00\\x00MJ\\x01j\\x98\\x01\\x00\\x00M,\\x01j\\x06\\x01\\x00\\x00K\\x96h\\xb0K:j\\x15\\x01\\x00\\x00K\\xa5jL\\x01\\x00\\x00K\\xdej\\xd1\\x01\\x00\\x00Me\\x01j\\x8c\\x01\\x00\\x00M\\x1e\\x01j\\xc0\\x01\\x00\\x00MT\\x01jF\\x01\\x00\\x00K\\xd8j\\x8d\\x01\\x00\\x00M\\x1f\\x01j\\x1f\\x01\\x00\\x00K\\xb0h\\xebK{h\\xd6Keh\\xd7Kfj\\xc2\\x01\\x00\\x00MV\\x01h\\xd8Kgh\\xd9KhuU\\x11content_encodings]r\\x12\\x02\\x00\\x00(]r\\x13\\x02\\x00\\x00(K\\x13K\\x1fK K\\x03K\\x17K\\xa6K\\x13K\\x03K\\x17K\\xa7KZK\\x03K\\x07K[K\\x03K\\\\K6K\\x01K\\x01K]K\\x18K^K!K\\x19K7K\\xa8K_K\\xa9K_K\\x01K\\x01K\\x01K\\x01K`K!K\\x19K7K\\xaaK\\x03K8K\\xabK\\x07K\\xacK\\xadK9K\\xaeK\\xafK8K6K:K;K!K\\x19K\\x04K\\x1aK\\xb0K\\nK\\x07K9K\\xb1K\\x05K\\x01K\\x01K\\x13K\\x1fK K\\x03K\\x17e]r\\x14\\x02\\x00\\x00(K\"K#K\\x01K\\x01K\\x01K\"K#K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\\x01K\"K#e]r\\x15\\x02\\x00\\x00(K$K%K\\x01K$K%K\\xb2KaK\\x10KbK\\xb3K\\x02K\\xb4K<K=K\\xb5KcKdK\\x08K&K&K\\x08K>K>K\\x08K\\'K>K\\x08K\\'K\\'K\\x08KeKeK\\x08KfKfK\\x08KgKgK\\x08KhKhK\\x08KiKiK\\x08KjKjK\\x08K\\xb6K\\xb7K\\xb8K\\xb9K\\xbaK\\xbbK\\x04K\\xbcK\\x05K\\xbdKkKlK\\xbeKkKlK\\xbfK\\xc0K<K=K\\xc1K\\xc2KmK\\xc3K\\xc4K\\xc5K\\xc6K\\xc7K\\xc8K\\xc9KnK?K@KmK!KnK\\xcaK\\xcbKAKBK\\xccK\\xcdK\\xceK\\x02K\\x1aK\\xcfK\\xd0K\\x04K\\xd1K\\x05K$K%e]r\\x16\\x02\\x00\\x00(KCKDK\\x01K\\x01K\\x01K\\x01K\\x01K\\xd2K\\x02KEKBK\\x19K&KEK\\x01K\\x01K\\x01K\\x01K\\x14KoK\\xd3K\\xd4K\\xd5K\\xd6K\\xd7K\\xd8K\\xd9K\\xdaK\\xdbK\\x01K\\xdcK:K\\xddK\\x01KFKGK\\xdeK\\x02K\\x14KFKGK(K\\x02K\\x14K\\x01KHKHK\\x01K(K\\x02K\\x14KCKDe]r\\x17\\x02\\x00\\x00(K\\x01KIKpK)KJK\\xdfK\\xe0K\\xe1K\\xe2K\\xe3K\\xe4K\\xe5K\\xe6K\\x01KHK\\x18K\\xe7KqK\\x02KJK\\x01K\\x01KrK\\xe8KsK\\xe9KsK\\x01K\\xeaK\\xebK\\x04K\\xecK\\xedK\\x05K\\x01K\\x01K\\x01K\\x01KIK)e]r\\x18\\x02\\x00\\x00(K*K\\x1bK\\xeeK\\x01K\\x1bK\\nK\\xefK*K\\x1bK?KtKuK\\xf0K\\x02K\\xf1K\\x02K\\xf2K\\x01K\\x01KvK\\xf3K\\x08K\\xf4K*K\\x1be]r\\x19\\x02\\x00\\x00(K+K\\x11K+K\\x11KKK\\x18KwK\\x02K\\xf5K\\xf6K\\x11KxK\\x07KLKMKNK,KyKLKMKNKAK,K,K@KOKzK\\xf7K\\x02K\\xf8K(K\\x1aK{K|K}K~K\\x7fKOKwK\\x02K\\x14K\\xf9K\\xfaK\\xfbK\\xfcK\\x80K\\x80K\\xfdK\\nK\\xfeK\\xffK\\x81M\\x00\\x01M\\x01\\x01M\\x02\\x01K\\x01K\\x01K+K\\x11e]r\\x1a\\x02\\x00\\x00(K\\x15K\\x1cKPM\\x03\\x01M\\x04\\x01K\\x15K\\x1cM\\x05\\x01KPK\\x07M\\x06\\x01K\\x02M\\x07\\x01K\\x15K\\x1cK\\x01M\\x08\\x01K\\x82K\\x10K\\x83M\\t\\x01K\\x02KQK\\x84M\\n\\x01K\\x02M\\x0b\\x01M\\x0c\\x01K\\x08K\\x07M\\r\\x01M\\x0e\\x01M\\x0f\\x01M\\x10\\x01M\\x11\\x01M\\x12\\x01M\\x13\\x01M\\x14\\x01M\\x15\\x01M\\x16\\x01M\\x17\\x01M\\x18\\x01K\\x85K\\x86K\\x15K:M\\x19\\x01M\\x1a\\x01K\\x87M\\x1b\\x01K\\x88M\\x1c\\x01M\\x1d\\x01M\\x1e\\x01M\\x1f\\x01K\\x15K\\x1ce]r\\x1b\\x02\\x00\\x00(K\\x01K\\x1aK\\x10K\\x89K\\x8aK\\x02K\\x8bK\\x8cK\\x8dM \\x01M!\\x01M\"\\x01M#\\x01KAM$\\x01KdM%\\x01M&\\x01K\\x01K\\x01M\\'\\x01M(\\x01K-K\\x8eK\\x8fK\\x90K\\x91K\\x92KQK\\x93K\\x94K\\x95K-K\\x96K\\x97K\\x98K\\x99e]r\\x1c\\x02\\x00\\x00(KRK.K\\x03K\\x16K\\x04M)\\x01M*\\x01K\\x05K]K\\x10M+\\x01K\\x9aK\\x18M,\\x01K\\x9aK\\x18M-\\x01K\\x9bK;M.\\x01M/\\x01KRK.K\\x03K\\x16M0\\x01M1\\x01M2\\x01K\\x02M3\\x01K\\x03M4\\x01K\\x16K\\x04K/K\\x05K\\x02M5\\x01M6\\x01K\\x04M7\\x01K\\x03M8\\x01K\\x05M9\\x01K\\x02M:\\x01K\\x02K\\x0bK\\x9cM;\\x01K\\x04M<\\x01M=\\x01K\\x05K\\x9dK\\x04K\\x9eK\\x05K\\x01K\\x13M>\\x01M?\\x01K\\x01K\\x01K\\x01K\\x01K/K\\x02K\\x9fK\\x01K\\x01K\\x01K.K\\x03K\\x16eeU\\x0flabel_encodings]r\\x1d\\x02\\x00\\x00(]r\\x1e\\x02\\x00\\x00(K\\x0cK\\x13K\\x1fK K\\x03K\\x17K\\x04M@\\x01KSK0K\\xa0K K\\x05K\\x0fK\\x07K9K\\x13K\\x03K\\x17K\\x0bKZK\\x03K\\x07K[K\\x03K\\\\K6K\\x04K^K\\x08K7K\\x05K\\x06K\\nMA\\x01K\\x02K\\x07MB\\x01K\\x03K`K\\x02MC\\x01MD\\x01ME\\x01K\\x02MF\\x01K\\xa0K\\x1dMG\\x01MH\\x01MI\\x01K\\x02MJ\\x01MK\\x01K\\x1dML\\x01K\\tK;K\\x0bMM\\x01MN\\x01MO\\x01K\\x1dK\\x07MP\\x01MQ\\x01MR\\x01K\\x06MS\\x01MT\\x01K\\x0fKPMU\\x01MV\\x01MW\\x01K\\x1dMX\\x01K\\xa1MY\\x01K\\x07MZ\\x01K\\x03K\\x07M[\\x01M\\\\\\x01K\\x06K\\re]r\\x1f\\x02\\x00\\x00(K\\x0cK\"K#K\\x0eK\\tM]\\x01KTM^\\x01M_\\x01KUK1M`\\x01K\\x06K\\re]r \\x02\\x00\\x00(K\\x0cK$K%K\\x04K\\x12KaK\\x10KbK\\x05K\\x0eK\\tMa\\x01K2Mb\\x01K\\x06K3Mc\\x01Md\\x01K0K\\tKcKVK<K=K\\x06K\\re]r!\\x02\\x00\\x00(K\\x0cKCKDK\\x02K\\x04K\\x12K\\x19KBK\\x02K&K\\x05K\\x0eK\\tK2Me\\x01KUMf\\x01Mg\\x01KEK\\x06K\\x1eMh\\x01K\\tMi\\x01K\\xa2Mj\\x01K\\x03K\\xa2KFKGK\\nK(K\\x14K\\x06K\\re]r\"\\x02\\x00\\x00(K\\x0cKIKpK)K\\x0eK\\tKTKrMk\\x01K\\x03K\\x07KJMl\\x01K\\x03Mm\\x01K\\x06K3K\\x0fK\\x12K\\nKqK\\x1dK\\x1fK\\x0bMn\\x01Mo\\x01K)K\\x06K\\re]r#\\x02\\x00\\x00(K\\x0cK*K\\x1bK\\x04K\\x12KtK?K\\x02KuK\\x05K\\x0eK\\xa3KWMp\\x01K\\x02Mq\\x01K\\x0bMr\\x01KvK\\x06K\\x1eK\\x0eMs\\x01K\\xa4KSKVMt\\x01K\\x07Mu\\x01Mv\\x01KKKXKXK\\x06K\\re]r$\\x02\\x00\\x00(K\\x0cK+Mw\\x01K\\x11K\\x04K\\x12KOK@K\\x02KzK\\x05K\\x02Mx\\x01My\\x01K\\x07Mz\\x01M{\\x01KXK\\x02K\\x0eK\\xa3KWK2K4K,KyKxK\\x07KLKMKNK\\x03KYK5K4K\\x04M|\\x01K\\x05K\\x06K3M}\\x01K\\x0bM~\\x01M\\x7f\\x01K\\x06K\\x11K\\x0fM\\x80\\x01M\\x81\\x01K\\x07K|K}K~K\\x7fK\\nK\\x07M\\x82\\x01M\\x83\\x01K\\x04M\\x84\\x01M\\x85\\x01K\\x05K\\x03K\\x07K\\'KYK5K4M\\x86\\x01K\\x06K3M\\x87\\x01M\\x88\\x01KYK5M\\x89\\x01K\\nK{K\\x06K\\x11M\\x8a\\x01M\\x8b\\x01K\\x81K\\x04M\\x8c\\x01K\\x05M\\x8d\\x01M\\x8e\\x01M\\x8f\\x01KoK4KUK\\x06K\\re]r%\\x02\\x00\\x00(K\\x0cK\\x15K\\x1cK\\x04K\\x12K\\x87M\\x90\\x01K\\x88KKK\\x82K\\x10K\\x83K\\x05K\\x0eK\\tM\\x91\\x01M\\x92\\x01K\\x0bM\\x93\\x01K\\x02K\\xa4KSK0K\\x07M\\x94\\x01K\\x03M\\x95\\x01K\\x84M\\x96\\x01K\\x85K\\x86K\\x06K\\re]r&\\x02\\x00\\x00(K\\x0cK\\x98K\\x99K\\x04K\\x12K\\x10K\\x1aK\\x02K\\x89K\\nK\\x8aK\\x02K\\x8bK\\x05K\\x0eK\\tKTK2KWK\\xa5K\\x8cK\\x8dKVK\\x07K\\x90K\\x91K\\x02K-K\\x8eK\\x8fK\\x02K\\x92KQK\\x93K\\x02K\\x94K\\x95K\\x02K\\x0bK-K\\x96K\\x97K\\x03K\\x07M\\x97\\x01K\\xa5K5K\\x06K\\re]r\\'\\x02\\x00\\x00(K\\x0cKRK.K\\x03K\\x16M\\x98\\x01K\\x04K\\x05K\\x0fK\\tK\\x9bM\\x99\\x01K\\x0bK\\x9cK1K\\x16K\\x04K/K\\x05K\\x06K\\x1eK\\x0fK\\tM\\x9a\\x01M\\x9b\\x01K1M\\x9c\\x01K\\x03M\\x9d\\x01M\\x9e\\x01K1K\\x9fK\\x04M\\x9f\\x01K\\x05K\\x06K\\x1eM\\xa0\\x01K\\tM\\xa1\\x01M\\xa2\\x01K\\x0bK\\x0fM\\xa3\\x01M\\xa4\\x01K0K\\tK8K\\nK/K\\x06K\\x1eK\\x0fK\\xa1K\\x9eK\\nK\\x9dK\\x06K\\reeU\\x0efield_rev_dict}r(\\x02\\x00\\x00(h\\x1cK\\x18h\\x05K\\x01h*K&hMKIhXKThWKShmKih>K:h\\x07K\\x03h\\x11K\\rhEKAhcK_hQKMh#K\\x1fh/K+h=K9h\\x13K\\x0fh)K%hDK@h\\rK\\th]KYh\\x1fK\\x1bh\\x19K\\x15h\\x06K\\x02h\\x0eK\\nh_K[h@K<h0K,hHKDh.K*hkKgh7K3h^KZh\\x0cK\\x08h\\x1aK\\x16hAK=h,K(h:K6h-K)h\\x0fK\\x0bh$K hhKdh&K\"h\\x15K\\x11h\\\\KXh(K$h6K2h!K\\x1dh5K1hRKNh\\x18K\\x14h\\tK\\x05h K\\x1chFKBhjKfhIKEh2K.hfKbh\\x12K\\x0eh`K\\\\h\\x0bK\\x07h?K;hgKch+K\\'hYKUhNKJhTKPh9K5heKah\\x1dK\\x19h\\x16K\\x12hBK>h;K7h[KWh4K0hSKOhlKhhLKHh1K-hdK`h\\x14K\\x10h\\x17K\\x13h\\x08K\\x04h8K4hKKGhUKQh\\x04K\\x00h\\x1bK\\x17h\\nK\\x06hGKCh%K!h\\'K#hVKRh<K8hOKKhaK]hiKehbK^h\\x1eK\\x1ahJKFhPKLh3K/h\\x10K\\x0chZKVh\"K\\x1ehCK?uu.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " open(plug_and_play_data_file, \"rb\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unpickle the processed data file and create the train_dev pratitions for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = unPickleIt(plug_and_play_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_encodings = data['field_encodings']\n",
    "field_dict = data['field_dict']\n",
    "\n",
    "content_encodings = data['content_encodings']\n",
    "\n",
    "label_encodings = data['label_encodings']\n",
    "content_label_dict = data['content_union_label_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a randomized cell that prints a complete sample to verify the sanity of the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table Contents: \n",
      "[('image', '<none>'), ('birthdate', '20'), ('birthdate', 'november'), ('birthdate', '1972'), ('birthplace', 'emporia'), ('birthplace', ','), ('birthplace', 'virginia'), ('position', 'defensive'), ('position', 'lineman'), ('number', '97'), ('college', 'north'), ('college', 'carolina'), ('heightft', '6'), ('heightin', '3'), ('weightlbs', '295'), ('undraftedyear', '1995'), ('stats', 'y'), ('databasefootball', 'parkerid01'), ('pfr', '<none>'), ('probowls', '<none>'), ('years', '1995\\xa01996-2000\\xa02001'), ('years', '2002-2003\\xa02004'), ('teams', 'san'), ('teams', 'diego'), ('teams', 'chargers'), ('teams', 'seattle'), ('teams', 'seahawks'), ('teams', 'new'), ('teams', 'england'), ('teams', 'patriots'), ('teams', 'baltimore'), ('teams', 'ravens'), ('teams', 'san'), ('teams', 'francisco'), ('teams', '49ers'), ('articletitle', 'riddick'), ('articletitle', 'parker')]\n",
      "\n",
      "\n",
      "Summary: \n",
      "['<start>', 'riddick', 'parker', '-lrb-', 'born', 'november', '20', ',', '1972', 'in', 'emporia', ',', 'virginia', '-rrb-', 'is', 'a', 'former', 'professional', 'american', 'football', 'defensive', 'lineman', 'for', 'the', 'seattle', 'seahawks', ',', 'san', 'diego', 'chargers', ',', 'new', 'england', 'patriots', ',', 'baltimore', 'ravens', ',', 'and', 'san', 'francisco', '49ers', 'of', 'the', 'national', 'football', 'league', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "total_samples = len(field_encodings)\n",
    "\n",
    "random_index = np.random.randint(total_samples)\n",
    "\n",
    "# extract the three parts of this random sample\n",
    "random_field_sample = field_encodings[random_index]\n",
    "content_sample = content_encodings[random_index]\n",
    "label_sample = label_encodings[random_index]\n",
    "\n",
    "# print the extracted sample in meaningful format\n",
    "print(\"Table Contents: \")\n",
    "print([(field_dict[field], content_label_dict[content]) \n",
    "       for (field, content) in zip(random_field_sample, content_sample)])\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Summary: \")\n",
    "print([content_label_dict[label] for label in label_sample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = list(zip(field_encodings, content_encodings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "run the above cell multiple times to satisfy yourself that the data is still sane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform random shuffling of the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'zip' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33640/662442974.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msynch_random_shuffle_non_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_encodings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Documents\\Data Science\\natural-language-summary-generation-from-structured-data-Animesh\\TensorFlow_implementation\\Summary_Generator\\Tensorflow_Graph\\utils.py\u001b[0m in \u001b[0;36msynch_random_shuffle_non_np\u001b[1;34m(X, Y)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# shuffle the combined list in place\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcombinedNew\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# extract the data back from the combined list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.shuffle\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'zip' has no len()"
     ]
    }
   ],
   "source": [
    "#X, Y = synch_random_shuffle_non_np(combined, label_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_new = list(zip(combined,label_encodings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(combined_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    " X, Y = zip(*combined_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform train_dev_splitting of the given data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, dev_X, dev_Y = split_train_dev(X, Y, train_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_field, train_X_content = zip(*train_X)\n",
    "train_X_field = list(train_X_field); train_X_content = list(train_X_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples in Training set:  9\n",
      "Number of Examples in the dev  set:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Examples in Training set: \", len(train_X))\n",
    "print(\"Number of Examples in the dev  set: \", len(dev_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up the resources by deleting non required stuff\n",
    "del X, Y, field_encodings, content_encodings, train_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building graph here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that the built graph will be later added to the code package Summary_Generator. This is being done here since the graph building process becomes quite easy with jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 0: Set the Hyper constants for the graph building process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also put all the summary_ops along with the graph. While executing the graph we can decide whether we wish to generate the summary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some hyper constants to be used in the graph building:\n",
    "\n",
    "# random_seed value for consistent debuggable behaviour\n",
    "seed_value = 3\n",
    "\n",
    "# vocabulary sizes\n",
    "field_vocab_size = data['field_vocab_size']\n",
    "content_label_vocab_size = data['content_label_vocab_size']\n",
    "\n",
    "# Embeddings size:\n",
    "field_embedding_size = 100\n",
    "content_label_embedding_size = 400 # This is a much bigger vocabulary compared to the field_name's vocabulary\n",
    "\n",
    "# LSTM hidden state sizes\n",
    "lstm_cell_state_size = hidden_state_size = 500 # they are same (for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph reset point:\n",
    "tf.compat.v1.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anusa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 1: Create placeholders for the computations in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders for the input data:\n",
    "with tf.compat.v1.variable_scope(\"Input_Data\"):\n",
    "    tf_field_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_field_encodings\")\n",
    "    tf_content_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_content_encodings\")\n",
    "    tf_label_encodings = tf.placeholder(tf.int32, shape=(None, None), name=\"input_label_encodings\")\n",
    "    \n",
    "    # This is a placeholder for storing the lengths of the input sequences (they are padded to tensor)\n",
    "    tf_input_seqs_lengths = tf.placeholder(tf.int32, shape=(None,), name=\"input_sequence_lengths\")\n",
    "    \n",
    "    # This is a placeholder for storing the lengths of the decoder sequences (they are padded to tensor)\n",
    "    tf_label_seqs_lengths = tf.placeholder(tf.int32, shape=(None,), name=\"decoder_sequence_lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the one-hot encoded values for the label_encodings\n",
    "with tf.variable_scope(\"One_hot_encoder\"):\n",
    "    tf_one_hot_label_encodings = tf.one_hot(tf_label_encodings, depth=content_label_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Input_Data/input_field_encodings:0\", shape=(?, ?), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# check tf_field_encodings\n",
    "print(tf_field_encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 2: Obtain Embeddings for the input and the output sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scope for the shared Content_Label matrix\n",
    "with tf.variable_scope(\"Unified_Vocabulary_Matrix\"):\n",
    "    content_label_embedding_matrix = tf.get_variable(\"content_label_embedding_matrix\", \n",
    "                                shape=(content_label_vocab_size, content_label_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings for the given input data:\n",
    "with tf.variable_scope(\"Input_Embedder\"):\n",
    "    # Embed the field encodings:\n",
    "    field_embedding_matrix = tf.get_variable(\"field_embedding_matrix\", \n",
    "                                shape=(field_vocab_size, field_embedding_size), \n",
    "                                initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                dtype=tf.float32)\n",
    "    \n",
    "    tf_field_embedded = tf.nn.embedding_lookup(field_embedding_matrix, tf_field_encodings, name=\"field_embedder\")\n",
    "    \n",
    "    # Embed the content encodings: \n",
    "    \n",
    "    \n",
    "    tf_content_embedded = tf.nn.embedding_lookup(content_label_embedding_matrix, \n",
    "                                                 tf_content_encodings, name=\"content_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded_Input_Tensors:  Tensor(\"Input_Embedder/field_embedder/Identity_1:0\", shape=(?, ?, 100), dtype=float32) Tensor(\"Input_Embedder/content_embedder/Identity_1:0\", shape=(?, ?, 400), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Input_Tensors: \", tf_field_embedded, tf_content_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings for the label (summary sentences):\n",
    "with tf.variable_scope(\"Label_Embedder\"):\n",
    "    # embed the label encodings\n",
    "    tf_label_embedded = tf.nn.embedding_lookup(content_label_embedding_matrix, \n",
    "                                                 tf_label_encodings, name=\"label_embedder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded_Label_Tensors:  Tensor(\"Label_Embedder/label_embedder/Identity_1:0\", shape=(?, ?, 400), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded_Label_Tensors: \", tf_label_embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the Input embeddings channel_wise and obtain the combined input tensor\n",
    "with tf.variable_scope(\"Input_Concatenator\"):\n",
    "    tf_field_content_embedded = tf.concat([tf_field_embedded, tf_content_embedded], axis=-1, name=\"concatenator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final_Input_to_the_Encoder:  Tensor(\"Input_Concatenator/concatenator:0\", shape=(?, ?, 500), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_Input_to_the_Encoder: \", tf_field_content_embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 3: Create the encoder RNN to obtain the encoded input sequences. <b>(The Encoder Module)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anusa\\AppData\\Local\\Temp/ipykernel_33640/3978288003.py:2: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\anusa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\layers\\rnn\\legacy_cells.py:1042: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anusa\\AppData\\Local\\Temp/ipykernel_33640/3978288003.py:3: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size), # let all parameters to be default\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Encoder\"):\n",
    "    encoded_input, encoder_final_state = tf.nn.dynamic_rnn (\n",
    "                            cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size), # let all parameters to be default\n",
    "                            inputs = tf_field_content_embedded,\n",
    "                            sequence_length = tf_input_seqs_lengths,\n",
    "                            dtype = tf.float32\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded_vectors_bank for attention mechanism:  Tensor(\"Encoder/rnn/transpose_1:0\", shape=(?, ?, 500), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Encoded_vectors_bank for attention mechanism: \", encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "# define the size parameter for the encoded_inputs\n",
    "encoded_inputs_embeddings_size = encoded_input.shape[-1]\n",
    "print(encoded_inputs_embeddings_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final_state obtained from the last step of encoder:  LSTMStateTuple(c=<tf.Tensor 'Encoder/rnn/while/Exit_3:0' shape=(?, 500) dtype=float32>, h=<tf.Tensor 'Encoder/rnn/while/Exit_4:0' shape=(?, 500) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Final_state obtained from the last step of encoder: \", encoder_final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4: define the Attention Mechanism for the Model <b>(The Dispatcher Module)</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.1: define the content based attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Content_Based_Attention/trainable_weights\"):\n",
    "    '''\n",
    "        These weights and bias matrices must be compatible with the dimensions of the h_values and the f_values\n",
    "        passed to the function below. If they are not, some exception might get thrown and it would be difficult\n",
    "        to debug it. \n",
    "    '''\n",
    "    # field weights for the content_based attention\n",
    "    W_f = tf.get_variable(\"field_attention_weights\", shape=(field_embedding_size, content_label_embedding_size),\n",
    "                         initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    b_f = tf.get_variable(\"field_attention_biases\", shape=(field_embedding_size, 1),\n",
    "                         initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    \n",
    "    # hidden states weights for the content_based attention\n",
    "    W_c = tf.get_variable(\"content_attention_weights\", \n",
    "                          shape=(encoded_inputs_embeddings_size, content_label_embedding_size),\n",
    "                          initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    b_c = tf.get_variable(\"content_attention_biases\", shape=(encoded_inputs_embeddings_size, 1),\n",
    "                          initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value))\n",
    "    \n",
    "    # Define the summary_ops for all the weights:\n",
    "    W_f_summary = tf.summary.histogram(\"Content_based_attention/field_weights\", W_f)\n",
    "    b_f_summary = tf.summary.histogram(\"Content_based_attention/field_biases\", b_f)\n",
    "    W_c_summary = tf.summary.histogram(\"Content_based_attention/content_weights\", W_c)\n",
    "    b_c_summary = tf.summary.histogram(\"Content_based_attention/content_weights\", b_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Content_Based_Attention\"):\n",
    "    def get_content_based_attention_vectors(query_vectors):\n",
    "        '''\n",
    "            function that returns the alpha_content vector using the yt-1 (query vectors)\n",
    "        '''\n",
    "        # use the W_f and b_f to transform the query_vectors to the shape of f_values\n",
    "        f_trans_query_vectors = tf.matmul(W_f, tf.transpose(query_vectors)) + b_f\n",
    "        # use the W_c and b_c to transform the query_vectors to the shape of h_values\n",
    "        h_trans_query_vectors = tf.matmul(W_c, tf.transpose(query_vectors)) + b_c\n",
    "        \n",
    "        # transpose and expand the dims of the f_trans_query_vectors\n",
    "        f_trans_query_matrices = tf.expand_dims(tf.transpose(f_trans_query_vectors), axis=-1)\n",
    "        # obtain the field attention_values by using the matmul operation\n",
    "        field_attention_values = tf.matmul(tf_field_embedded, f_trans_query_matrices)\n",
    "        \n",
    "        # perform the same process for the h_trans_query_vectors\n",
    "        h_trans_query_matrices = tf.expand_dims(tf.transpose(h_trans_query_vectors), axis=-1)\n",
    "        hidden_attention_values = tf.matmul(encoded_input, h_trans_query_matrices)\n",
    "        \n",
    "        # drop the last dimension (1 sized)\n",
    "        field_attention_values = tf.squeeze(field_attention_values, axis=[-1])\n",
    "        hidden_attention_values = tf.squeeze(hidden_attention_values, axis=[-1]) # same for this one\n",
    "        \n",
    "        # return the element wise multiplied values followed by softmax\n",
    "        return tf.nn.softmax(field_attention_values * hidden_attention_values, name=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.2: define the link based attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Link_Based_Attention/trainable_weights\"):\n",
    "    '''\n",
    "        The dimensions of the Link_Matrix must be properly compatible with the field_vocab_size.\n",
    "        If they are not, some exception might get thrown and it would be difficult\n",
    "        to debug it.\n",
    "    '''\n",
    "    Link_Matrix = tf.get_variable(\"Link_Attention_Matrix\", shape=(field_vocab_size, field_vocab_size),\n",
    "            dtype=tf.float32, initializer=tf.truncated_normal_initializer(mean=0.5, stddev=0.5, seed=seed_value))\n",
    "    \n",
    "    Link_Matrix_summary = tf.summary.histogram(\"Link_based_attention\", Link_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Link_Based_Attention/trainable_weights/Link_Attention_Matrix:0' shape=(106, 106) dtype=float32>\n"
     ]
    }
   ],
   "source": [
    "print(Link_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the function for obtaining the link based attention values.\n",
    "with tf.variable_scope(\"Link_Based_Attention\"):\n",
    "    def get_link_based_attention_vectors(prev_attention_vectors):\n",
    "        '''\n",
    "            This function generates the link based attention vectors using the Link matrix and the \n",
    "        '''\n",
    "        # carve out only the relevant values from the Link matrix\n",
    "        matrix_all_values_from = tf.nn.embedding_lookup(Link_Matrix, tf_field_encodings)\n",
    "        \n",
    "        # // TODO: Calculate the matrix_relevant_values from matrix_all_values_from\n",
    "        matrix_relevant_values = tf.map_fn(lambda u: tf.gather(u[0],u[1],axis=1),\n",
    "                                [matrix_all_values_from, tf_field_encodings], dtype=matrix_all_values_from.dtype)\n",
    "        \n",
    "        \n",
    "        return tf.nn.softmax(tf.reduce_sum(tf.expand_dims(prev_attention_vectors, axis = -1) * \n",
    "                                           matrix_relevant_values, axis=-1),name=\"softmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 4.3: define the hybrid attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the hybrid of the content based and the link based attention\n",
    "with tf.variable_scope(\"Hybrid_attention/trainable_weights\"):\n",
    "    # for now, this is just the content_based attention:\n",
    "    Zt_weights = tf.get_variable(\"zt_gate_parameter_vector\", dtype=tf.float32,\n",
    "                                 initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                                 shape=(hidden_state_size + field_embedding_size + content_label_embedding_size, 1))\n",
    "    \n",
    "    Zt_weights_summary = tf.summary.histogram(\"Hybrid_attention/zt_weights\", Zt_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Hybrid_attention\"):\n",
    "    # define the hybrid_attention_calculator function:\n",
    "    def get_hybrid_attention(h_values, y_values, content_attention, link_attention):\n",
    "        '''\n",
    "            function to calcuate the hybrid attention using the content_attention and the link_attention\n",
    "        '''\n",
    "        # calculate the e_f values\n",
    "        e_t = tf.reduce_sum(tf.expand_dims(link_attention, axis=-1) * tf_field_embedded, axis=1)\n",
    "        \n",
    "        # create the concatenated vectors from h_values e_t and y_values\n",
    "        input_to_zt_gate = tf.concat([h_values, e_t, y_values], axis=-1) # channel wise concatenation\n",
    "        \n",
    "        # perfrom the computations of the z gate:\n",
    "        z_t = tf.nn.sigmoid(tf.matmul(input_to_zt_gate, Zt_weights))\n",
    "        \n",
    "        # calculate z_t~ value using the empirical values = 0.2z_t + 0.5\n",
    "        z_t_tilde = (0.2 * z_t) + 0.5\n",
    "        \n",
    "        # compute the final hybrid_attention_values using the z_t_tilde values over content and link based values\n",
    "        hybrid_attention = (z_t_tilde * content_attention) + ((1 - z_t_tilde) * link_attention)\n",
    "        \n",
    "        # return the calculated hybrid attention:\n",
    "        return hybrid_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step 5: create the decoder RNN to obtain the generated summary for the structured data <b>(The Decoder Module)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anusa\\AppData\\Local\\Temp/ipykernel_33640/807845370.py:29: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  decoder_cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size)\n"
     ]
    }
   ],
   "source": [
    " with tf.variable_scope(\"Decoder/trainable_weights\"):\n",
    "        # define the weights for the output projection calculation\n",
    "        W_output = tf.get_variable(\n",
    "                            \"output_projector_matrix\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(hidden_state_size, content_label_vocab_size))\n",
    "        b_output = tf.get_variable(\n",
    "                            \"output_projector_biases\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(content_label_vocab_size,))\n",
    "        \n",
    "        # define the weights and biases for the x_t calculation\n",
    "        W_d = tf.get_variable(\n",
    "                        \"x_t_gate_matrix\", dtype=tf.float32,\n",
    "                        initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                        shape=((hidden_state_size + content_label_embedding_size), content_label_embedding_size))\n",
    "        b_d = tf.get_variable(\n",
    "                            \"x_t_gate_biases\", dtype=tf.float32,\n",
    "                            initializer=tf.random_uniform_initializer(minval=-1, maxval=1, seed=seed_value),\n",
    "                            shape=(content_label_embedding_size,))\n",
    "        \n",
    "        # define the summary ops for the defined weights and biases\n",
    "        W_output_summary = tf.summary.histogram(\"Decoder/W_output\", W_output)\n",
    "        b_output_summary = tf.summary.histogram(\"Decoder/b_output\", b_output)\n",
    "        W_d_summary = tf.summary.histogram(\"Decoder/W_d\", W_d)\n",
    "        b_d_summary = tf.summary.histogram(\"Decoder/b_d\", b_d)\n",
    "        \n",
    "        # create the LSTM cell to be used for decoding purposes\n",
    "        decoder_cell = tf.nn.rnn_cell.LSTMCell(lstm_cell_state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(start_tokens, mode = \"inference\", decoder_lengths = None, w_reuse = True):\n",
    "    '''\n",
    "        Function that defines the decoder op and returns the decoded sequence (the summary)\n",
    "        \n",
    "        @params:\n",
    "        start_tokens = a tensor containing the start tokens (one for each sequence in the batch)\n",
    "        mode = a value from \"training\" or \"inference\" to determine for how long the decoder rnn is to be unrolled.\n",
    "               behaviour is as follows:\n",
    "               \"training\" => The rnn will be unrolled until the max(decode_lengths). decode_lengths cannot be None.\n",
    "               \"inference\" => decode_lengths is be ignored and unrolling will be done till <eos> is received\n",
    "               \n",
    "    '''\n",
    "    with tf.variable_scope(\"Decoder\", reuse = w_reuse):\n",
    "        # define the function to obtain the predictions out of the given hidden_state_values\n",
    "        def get_predictions(h_t_values):\n",
    "            '''\n",
    "                This function transforms the h_t_values into a one_hot_type probability vector\n",
    "            '''\n",
    "            # apply the output_projection gate to obtain the predictions from the h_t_values\n",
    "            predictions = tf.matmul(h_t_values, W_output) + b_output\n",
    "            \n",
    "            # return the predictions:\n",
    "            return predictions\n",
    "        \n",
    "        \n",
    "        # define a function to obtain the values for the next input to the LSTM_cell (y_t values)\n",
    "        def get_y_t_values(pred_vals):\n",
    "            '''\n",
    "                pred_vals = the tensor of shape [batch_size x content_label_vocab_size]\n",
    "            '''\n",
    "            \n",
    "            # calculate the next words to be predicted \n",
    "            act_preds = tf.argmax(pred_vals, axis=-1)\n",
    "            \n",
    "            # perform embedding lookup for these act_preds\n",
    "            y_t_values = tf.nn.embedding_lookup(content_label_embedding_matrix, act_preds)\n",
    "            \n",
    "            # return the calculated y_t_values\n",
    "            return y_t_values\n",
    "            \n",
    "        \n",
    "        # write the loop function for the raw_rnn:\n",
    "        def decoder_loop_function(time, cell_output, cell_state, loop_state):\n",
    "            '''\n",
    "                The decoder loop function for the raw_rnn\n",
    "                (In future will implement the attention mechanism using the loop_state parameter.)\n",
    "                @params\n",
    "                compatible with -> https://www.tensorflow.org/api_docs/python/tf/nn/raw_rnn\n",
    "            '''\n",
    "            if(cell_state is None):\n",
    "                # initial call of the loop function\n",
    "                finished = (time >= tf_label_seqs_lengths)\n",
    "                next_input = start_tokens\n",
    "                next_cell_state = encoder_final_state\n",
    "                emit_output = tf.placeholder(tf.float32, shape=(content_label_vocab_size))\n",
    "                next_loop_state = tf.zeros_like(tf_field_encodings, dtype=tf.float32)\n",
    "                \n",
    "            else:\n",
    "                # we define the loop_state as the prev_hybrid attention_vector!\n",
    "                prev_attention_vectors = loop_state # extract the prev_attention_vector from the loop state\n",
    "                \n",
    "                # obtain the predictions for the cell_output\n",
    "                preds = get_predictions(cell_output)\n",
    "                \n",
    "                # obtain the y_t_values from the cell_output values:\n",
    "                y_t_values = get_y_t_values(preds)\n",
    "                \n",
    "                ''' Calculate the attention: '''\n",
    "                # calculate the content_based attention values using the defined module\n",
    "                cont_attn = get_content_based_attention_vectors(y_t_values)\n",
    "                \n",
    "                # calculate the link based attention values\n",
    "                link_attn = get_link_based_attention_vectors(prev_attention_vectors)\n",
    "                # print \"link_attention: \", link_attn\n",
    "                \n",
    "                # calculate the hybrid_attention\n",
    "                hybrid_attn = get_hybrid_attention(cell_output, y_t_values, cont_attn, link_attn)\n",
    "                \n",
    "                ''' Calculate the x_t vector for next_input value'''\n",
    "                # use the hybrid_attn to attend over the encoded_input (to calculate the a_t values)\n",
    "                a_t_values = tf.reduce_sum(tf.expand_dims(hybrid_attn, axis=-1) * encoded_input, axis=1) \n",
    "                \n",
    "                # apply the x_t gate\n",
    "                x_t = tf.tanh(tf.matmul(tf.concat([a_t_values, y_t_values], axis=-1), W_d) + b_d)\n",
    "                \n",
    "                \n",
    "                ''' Calculate the finished vector for perfoming computations '''\n",
    "                # for now it is just the decoder length completed or not value.\n",
    "                finished = (time >= decoder_lengths)\n",
    "                \n",
    "                ''' Copy mechanism is left (//TODO: change the following and implement copy mechanism)'''\n",
    "                emit_output = preds\n",
    "                \n",
    "                # The next_input is the x_t vector so calculated:\n",
    "                next_input = x_t\n",
    "                # The next loop_state is the current hybrid_attention vectors\n",
    "                next_loop_state = cont_attn\n",
    "                # The next_cell_state is going to be equal to the cell_state. (we_don't tweak it)\n",
    "                next_cell_state = cell_state\n",
    "            \n",
    "            # In both the cases, the return value is same.\n",
    "            # return all these created parameters\n",
    "            return (finished, next_input, next_cell_state, emit_output, next_loop_state)\n",
    "        \n",
    "        # use the tf.nn.raw_rnn to define the decoder computations\n",
    "        outputs, _, _ = tf.nn.raw_rnn(decoder_cell, decoder_loop_function)\n",
    "        \n",
    "    # return the outputs obtained from the raw_rnn:\n",
    "    return tf.transpose(outputs.stack(), perm=[1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "step 6: define the training computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anusa\\AppData\\Local\\Temp/ipykernel_33640/1744963724.py:11: calling map_fn (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"Training_computations\"):\n",
    "    outputs = decode(tf_label_embedded[:, 0, :], mode=\"training\", \n",
    "                     decoder_lengths=tf_label_seqs_lengths, w_reuse=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output_tensor:  Tensor(\"Training_computations/transpose:0\", shape=(?, ?, 421), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# print the outputs:\n",
    "print(\"Output_tensor: \", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "step 7: define the cost function and the optimizer to perform the optimization on this graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\anusa\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define the loss (objective) function for minimization\n",
    "with tf.variable_scope(\"Loss\"):\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=tf_one_hot_label_encodings))\n",
    "    \n",
    "    # record the loss summary:\n",
    "    loss_summary = tf.summary.scalar(\"Objective_loss\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimizer for this task:\n",
    "with tf.variable_scope(\"Trainer\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    # define the train_step for this:\n",
    "    train_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step _ : define the errands for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Errands\"): \n",
    "    init = tf.global_variables_initializer()\n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a stub_session to generate the graph visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Model_1\"\n",
    "model_path = os.path.join(base_model_path, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified_Vocabulary_Matrix/content_label_embedding_matrix:0\n",
      "Input_Embedder/field_embedding_matrix:0\n",
      "Encoder/rnn/lstm_cell/kernel:0\n",
      "Encoder/rnn/lstm_cell/bias:0\n",
      "Content_Based_Attention/trainable_weights/field_attention_weights:0\n",
      "Content_Based_Attention/trainable_weights/field_attention_biases:0\n",
      "Content_Based_Attention/trainable_weights/content_attention_weights:0\n",
      "Content_Based_Attention/trainable_weights/content_attention_biases:0\n",
      "Link_Based_Attention/trainable_weights/Link_Attention_Matrix:0\n",
      "Hybrid_attention/trainable_weights/zt_gate_parameter_vector:0\n",
      "Decoder/trainable_weights/output_projector_matrix:0\n",
      "Decoder/trainable_weights/output_projector_biases:0\n",
      "Decoder/trainable_weights/x_t_gate_matrix:0\n",
      "Decoder/trainable_weights/x_t_gate_biases:0\n",
      "Decoder/rnn/lstm_cell/kernel:0\n",
      "Decoder/rnn/lstm_cell/bias:0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tensorboard_writer = tf.summary.FileWriter(model_path, graph=sess.graph, filename_suffix=\".bot\")\n",
    "    \n",
    "    # initialize the session to generate the visualization file\n",
    "    sess.run(init)\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    tvars_vals = sess.run(tvars)\n",
    "    \n",
    "    for var, val in zip(tvars, tvars_vals):\n",
    "        print(var.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the session runner to check if the training loops execute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell ensures that although there are no errors in the graph compilation, the runtime execution of the model also doesn't cause any problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.plugins import projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the projector's configuration to add the embedding summary also:\n",
    "conf = projector.ProjectorConfig()\n",
    "embedding_field = conf.embeddings.add()\n",
    "embedding_content_label = conf.embeddings.add()\n",
    "\n",
    "# set the tensors to these embedding matrices\n",
    "embedding_field.tensor_name = field_embedding_matrix.name\n",
    "embedding_content_label.tensor_name = content_label_embedding_matrix.name\n",
    "\n",
    "# add the metadata paths to these embedding_summaries:\n",
    "embedding_field.metadata_path = os.path.join(\"..\", \"Metadata/fields.vocab\")\n",
    "embedding_content_label.metadata_path = os.path.join(\"..\", \"Metadata/content_labels.vocab\")\n",
    "\n",
    "# save the configuration file for this\n",
    "projector.visualize_embeddings(tensorboard_writer, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_epoch:  1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33640/2267233288.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"current_epoch: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# execute the cost and the train_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         predicts, _, cost = sess.run([outputs, train_step, loss], feed_dict = {\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[0mtf_field_encodings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minp_field\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mtf_content_encodings\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minp_conte\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 968\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    969\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    970\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1159\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1161\u001b[1;33m             \u001b[0mnp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1163\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'map'"
     ]
    }
   ],
   "source": [
    "''' The following is just a runtime checker session loop. This loop is not the training loop for the model.\n",
    "Which is the reason why, the model is not saved upon executing'''\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # create a saver object:\n",
    "    saver = tf.train.Saver(max_to_keep=3)\n",
    "    \n",
    "    if(os.path.isfile(os.path.join(model_path, \"checkpoint\"))):\n",
    "        # load the saved weights:\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(model_path))\n",
    "    else:\n",
    "        # run the initializer to create the variables\n",
    "        sess.run(init)\n",
    "    \n",
    "    # obtain the padded training data:\n",
    "    inp_field = pad_sequences(train_X_field)\n",
    "    inp_conte = pad_sequences(train_X_content)\n",
    "    inp_label = pad_sequences(train_Y)\n",
    "    # print inp_field.shape, inp_conte.shape, inp_label.shape\n",
    "    \n",
    "    # obtain the sequence lengths for the field_encodings and the label_encodings\n",
    "    inp_lengths = get_lengths(train_X_field)\n",
    "    lab_lengths = get_lengths(train_Y)\n",
    "    # print inp_lengths, lab_lengths\n",
    "    \n",
    "    # run a loop for 1000 iterations:\n",
    "    for epoch in range(1000):\n",
    "        print(\"current_epoch: \", (epoch + 1))\n",
    "        # execute the cost and the train_step\n",
    "        predicts, _, cost = sess.run([outputs, train_step, loss], feed_dict = {\n",
    "            tf_field_encodings: inp_field,\n",
    "            tf_content_encodings: inp_conte,\n",
    "            tf_label_encodings: inp_label,\n",
    "            tf_input_seqs_lengths: inp_lengths,\n",
    "            tf_label_seqs_lengths: lab_lengths\n",
    "        })\n",
    "        \n",
    "        if((epoch + 1) % 10 == 0 or epoch == 0):\n",
    "            # generate the summary for this batch:\n",
    "            sums = sess.run(all_summaries, feed_dict = {\n",
    "                tf_field_encodings: inp_field,\n",
    "                tf_content_encodings: inp_conte,\n",
    "                tf_label_encodings: inp_label,\n",
    "                tf_input_seqs_lengths: inp_lengths,\n",
    "                tf_label_seqs_lengths: lab_lengths\n",
    "            })\n",
    "            \n",
    "            # save this generated summary to the summary file\n",
    "            tensorboard_writer.add_summary(sums, global_step=(epoch + 1))\n",
    "            \n",
    "            # also save the model \n",
    "            saver.save(sess, os.path.join(model_path, model_name), global_step=(epoch + 1))\n",
    "            \n",
    "        print(\"Cost: \", cost, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
