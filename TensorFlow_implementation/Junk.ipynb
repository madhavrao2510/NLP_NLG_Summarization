{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa27cb2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Tensorflow_Graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13004/4186586122.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mSummary_Generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mText_Preprocessing_Helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpickling_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mSummary_Generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensorflow_Graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mSummary_Generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\Data Science\\natural-language-summary-generation-from-structured-data-Animesh\\TensorFlow_implementation\\Summary_Generator\\Model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#from tensorflow.contrib.tensorboard.plugins import projector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplugins\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprojector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mTensorflow_Graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Tensorflow_Graph'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Script for checking if the Inference computations run properly for the trained graph.\n",
    "'''\n",
    "\n",
    "from Summary_Generator.Tensorflow_Graph import order_planner_without_copynet\n",
    "from Summary_Generator.Text_Preprocessing_Helpers.pickling_tools import *\n",
    "from Summary_Generator.Tensorflow_Graph.utils import *\n",
    "from Summary_Generator.Model import *\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64cadea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the class for the Model\n",
    "class Model:\n",
    "    '''\n",
    "        Model for training and Inferring on the task\n",
    "        This is a simplified implementation of the Tensorflow's estimator module\n",
    "    '''\n",
    "    # Helper methods for initialization:\n",
    "    def __setup_optimizer(self):\n",
    "        # setup the optimizer with the graph\n",
    "        with self.graph.as_default():\n",
    "            # define the optimizer for this task:\n",
    "        \twith tf.variable_scope(\"Trainer\"):\n",
    "        \t    # define the train_step for this:\n",
    "        \t    self.train_step = self.optimizer.minimize(self.loss)\n",
    "\n",
    "        \twith tf.variable_scope(\"Init\"):\n",
    "        \t\tself.init = tf.global_variables_initializer()\n",
    "\n",
    "    def __setup_graph(self):\n",
    "        # print all the trainable variables for this graph\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            # initialize the session to generate the visualization file\n",
    "            sess.run(self.init)\n",
    "\n",
    "            tvars = tf.trainable_variables()\n",
    "            tvars_vals = sess.run(tvars)\n",
    "\n",
    "            print(\"\\n\\nAll the trainable variables in the Graph: \")\n",
    "            for var, val in zip(tvars, tvars_vals):\n",
    "                print(var.name)\n",
    "            print(\"\\n\\n\")\n",
    "\n",
    "    def __get_tensorboard_writer(self, path):\n",
    "        tensorboard_writer = tf.summary.FileWriter(path, graph=self.graph, filename_suffix=\".bot\")\n",
    "\n",
    "        # set the projector's configuration to add the embedding summary also:\n",
    "        conf = projector.ProjectorConfig()\n",
    "        embedding_field = conf.embeddings.add()\n",
    "        embedding_content_label = conf.embeddings.add()\n",
    "\n",
    "        # set the tensors to these embedding matrices\n",
    "        embedding_field.tensor_name = self.field_embedding_matrix.name\n",
    "        embedding_content_label.tensor_name = self.content_label_embedding_matrix.name\n",
    "\n",
    "        # add the metadata paths to these embedding_summaries:\n",
    "        embedding_field.metadata_path = os.path.join(\"..\", \"Metadata/fields.vocab\")\n",
    "        embedding_content_label.metadata_path = os.path.join(\"..\", \"Metadata/content_labels.vocab\")\n",
    "\n",
    "        # save the configuration file for this\n",
    "        projector.visualize_embeddings(tensorboard_writer, conf)\n",
    "\n",
    "        # return the so created tensorboard_writer\n",
    "        return tensorboard_writer\n",
    "\n",
    "    # define the constructor of the graph\n",
    "    def __init__(self, graph, interface_dict, optimizer, field_vocabulary, content_label_vocabulary):\n",
    "        '''\n",
    "            constructor of the class\n",
    "\n",
    "            graph = The tensorflow graph of the network\n",
    "            optimizer = The tensorflow optimizer object that is to be used for optimization\n",
    "        '''\n",
    "        self.graph = graph\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # extract the parameters from the interface_dict:\n",
    "        self.loss = interface_dict[\"loss\"]\n",
    "        self.all_summaries = interface_dict[\"summary\"]\n",
    "        self.outputs = interface_dict[\"training_output\"]\n",
    "        self.inference = interface_dict[\"inference\"]\n",
    "\n",
    "        self.inp_field_encodings = interface_dict[\"input\"][\"field_encodings\"]\n",
    "        self.inp_content_encodings = interface_dict[\"input\"][\"content_encodings\"]\n",
    "        self.inp_label_encodings = interface_dict[\"input\"][\"label_encodings\"]\n",
    "        self.inp_sequence_lengths = interface_dict[\"input\"][\"input_sequence_lengths\"]\n",
    "        self.lab_sequence_lengths = interface_dict[\"input\"][\"label_sequence_lengths\"]\n",
    "\n",
    "        self.field_embedding_matrix = interface_dict[\"field_embeddings\"]\n",
    "        self.content_label_embedding_matrix = interface_dict[\"content_label_embeddings\"]\n",
    "\n",
    "        # setup the vocabularies for the Model:\n",
    "        self.field_vocabulary = field_vocabulary\n",
    "        self.content_label_vocabulary = content_label_vocabulary\n",
    "\n",
    "        self.__setup_optimizer()\n",
    "        self.__setup_graph()\n",
    "\n",
    "    # function for training the graph\n",
    "    def train(self, X, Y, batch_size, no_of_epochs, checkpoint_factor, model_save_path, model_name):\n",
    "        '''\n",
    "            The training_function for the model.\n",
    "        '''\n",
    "        # Setup a tensorboard_writer:\n",
    "        tensorboard_writer = self.__get_tensorboard_writer(model_save_path)\n",
    "\n",
    "        ''' Start the actual Training loop: '''\n",
    "        print(\"\\n\\nStarting the Training ... \")\n",
    "        with tf.Session(graph=self.graph) as sess:\n",
    "            # create a saver object:\n",
    "            saver = tf.train.Saver(max_to_keep=3)\n",
    "\n",
    "            # If old weights found, restart the training from there:\n",
    "            checkpoint_file = os.path.join(model_save_path, \"checkpoint\")\n",
    "            if(os.path.isfile(checkpoint_file)):\n",
    "                # load the saved weights:\n",
    "                saver.restore(sess, tf.train.latest_checkpoint(model_save_path))\n",
    "\n",
    "                # load the global_step value from the checkpoint file\n",
    "                with open(checkpoint_file, 'r') as checkpoint:\n",
    "                    path = checkpoint.readline().strip()\n",
    "                    global_step = int((path.split(':')[1]).split('-')[1][:-1])\n",
    "\n",
    "            # otherwise initialize all the weights\n",
    "            else:\n",
    "                sess.run(self.init)\n",
    "\n",
    "                # set the global_step to 0\n",
    "                global_step = 0\n",
    "\n",
    "            print(\"global_step: \", global_step)\n",
    "\n",
    "            # run a loop for no_of_epochs iterations:\n",
    "            for epoch in range(no_of_epochs):\n",
    "                print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "                print(\"current_epoch: \", (epoch + 1))\n",
    "\n",
    "                # perform random shuffling on the training data:\n",
    "                X, Y = synch_random_shuffle_non_np(zip(X[0], X[1]), Y)\n",
    "\n",
    "                # unzip the shuffled X:\n",
    "                X = zip(*X)\n",
    "\n",
    "                # setup the data for training:\n",
    "                # obtain the padded training data:\n",
    "                train_X_field = X[0]; train_X_content = X[1]\n",
    "                train_Y = Y; no_of_total_examples = len(train_X_field)\n",
    "\n",
    "                # print len(train_X_field), len(train_X_content), len(train_Y)\n",
    "                assert len(train_X_field) == len(train_X_content) and len(train_X_field) == len(train_Y), \"input data lengths incompatible\"\n",
    "\n",
    "                # Iterate over the batches of the given train data:\n",
    "                for batch_no in range(int(np.ceil(float(no_of_total_examples) / batch_size))):\n",
    "                    # obtain the current batch of data:\n",
    "                    start = (batch_no * batch_size); end = start + batch_size\n",
    "                    batch_inp_field = train_X_field[start: end]\n",
    "                    batch_inp_conte = train_X_content[start: end]\n",
    "                    batch_inp_label = train_Y[start: end]\n",
    "                    # pad the current batch of data:\n",
    "                    inp_field = pad_sequences(batch_inp_field)\n",
    "                    inp_conte = pad_sequences(batch_inp_conte)\n",
    "                    inp_label = pad_sequences(batch_inp_label)\n",
    "                    # extract the sequence lengths of examples in this batch\n",
    "                    inp_lengths = get_lengths(batch_inp_field)\n",
    "                    lab_lengths = get_lengths(batch_inp_label)\n",
    "\n",
    "\n",
    "                    # execute the cost and the train_step\n",
    "                    _, cost = sess.run([self.train_step, self.loss], feed_dict = {\n",
    "                        self.inp_field_encodings: inp_field,\n",
    "                        self.inp_content_encodings: inp_conte,\n",
    "                        self.inp_label_encodings: inp_label,\n",
    "                        self.inp_sequence_lengths: inp_lengths,\n",
    "                        self.lab_sequence_lengths: lab_lengths\n",
    "                    })\n",
    "                    print(\"Range: \", \"[\", start, \"-\", (start + len(inp_field)), \"]\", \" Cost: \", cost)\n",
    "                    global_step += 1\n",
    "\n",
    "                if((epoch + 1) % checkpoint_factor == 0 or epoch == 0):\n",
    "                    # generate the summary for this batch:\n",
    "                    sums, predicts = sess.run([self.all_summaries, self.outputs], feed_dict = {\n",
    "                        self.inp_field_encodings: inp_field,\n",
    "                        self.inp_content_encodings: inp_conte,\n",
    "                        self.inp_label_encodings: inp_label,\n",
    "                        self.inp_sequence_lengths: inp_lengths,\n",
    "                        self.lab_sequence_lengths: lab_lengths\n",
    "                    })\n",
    "\n",
    "                    # save this generated summary to the summary file\n",
    "                    tensorboard_writer.add_summary(sums, global_step=global_step)\n",
    "\n",
    "                    # also save the model\n",
    "                    saver.save(sess, os.path.join(model_save_path, model_name), global_step=global_step)\n",
    "\n",
    "                    # print a random sample from this batch:\n",
    "                    random_index = np.random.randint(len(inp_field))\n",
    "\n",
    "                    random_label_sample = inp_label[random_index]\n",
    "                    random_predicts_sample = np.argmax(predicts, axis = -1)[random_index]\n",
    "\n",
    "                    # print the extracted sample in meaningful format\n",
    "                    print(\"\\nOriginal Summary: \")\n",
    "                    print([self.content_label_vocabulary[label] for label in random_label_sample])\n",
    "\n",
    "                    print(\"\\nPredicted Summary: \")\n",
    "                    print([self.content_label_vocabulary[label] for label in random_predicts_sample])\n",
    "\n",
    "                print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Training complete ...\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1276ca68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
